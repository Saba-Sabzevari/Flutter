{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyP50b2C3CQOOp2je0hpUdO4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Saba-Sabzevari/Flutter/blob/main/Precision_Agriculture_Soil_Health_Classification_System.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Precision Agriculture Soil Health Classification System\n",
        "\n",
        "## Project Overview\n",
        "- **Course:** Pattern Recognition (M.Sc.) - WS25\n",
        "- **Dataset:** Kaggle Comprehensive Soil Classification Dataset\n",
        "- **Research Questions:** 5 (RQ1-RQ5)\n",
        "\n",
        "---\n",
        "\n",
        "## List of Figures and Tables Generated by This Notebook\n",
        "\n",
        "### Figures (24 PDFs)\n",
        "\n",
        "#### Methodology Section\n",
        "| Figure | Filename | Description |\n",
        "|--------|----------|-------------|\n",
        "| Fig 1 | `dataset_samples.pdf` | Sample images from each of the 7 soil classes |\n",
        "| Fig 2 | `dataset_distribution.pdf` | Bar chart showing class distribution with train/val/test split |\n",
        "\n",
        "#### RQ1 - CNN Architecture Comparison\n",
        "| Figure | Filename | Description |\n",
        "|--------|----------|-------------|\n",
        "| Fig 3 | `RQ1/RQ1_Fig1.pdf` | Confusion Matrix - ResNet50 |\n",
        "| Fig 4 | `RQ1/RQ1_Fig2.pdf` | Confusion Matrix - EfficientNet-B3 |\n",
        "| Fig 5 | `RQ1/RQ1_Fig3.pdf` | Training Curves (Loss & Accuracy) |\n",
        "| Fig 6 | `RQ1/RQ1_Fig4.pdf` | Confusion Matrix - Vision Transformer (ViT) |\n",
        "| Fig 7 | `RQ1/RQ1_Fig5.pdf` | Per-Class F1-Score Comparison |\n",
        "| Fig 8 | `RQ1/RQ1_Fig6.pdf` | ROC Curves for all 7 classes |\n",
        "\n",
        "#### RQ2 - Metalearner Ensemble\n",
        "| Figure | Filename | Description |\n",
        "|--------|----------|-------------|\n",
        "| Fig 9 | `RQ2/RQ2_Fig1.pdf` | Confusion Matrix - Metalearner |\n",
        "| Fig 10 | `RQ2/RQ2_Fig2.pdf` | F1-Score Comparison (all models) |\n",
        "| Fig 11 | `RQ2/RQ2_Fig3.pdf` | Base Learner Performance Comparison |\n",
        "\n",
        "#### RQ3 - Hybrid Expert System\n",
        "| Figure | Filename | Description |\n",
        "|--------|----------|-------------|\n",
        "| Fig 12 | `RQ3/RQ3_Fig1.pdf` | Confusion Matrix - Hybrid System |\n",
        "| Fig 13 | `RQ3/RQ3_Fig2.pdf` | Accuracy Comparison Bar Chart |\n",
        "| Fig 14 | `RQ3/RQ3_Fig3.pdf` | Rule Application Distribution |\n",
        "\n",
        "#### RQ4 - Explainability\n",
        "| Figure | Filename | Description |\n",
        "|--------|----------|-------------|\n",
        "| Fig 15 | `RQ4/RQ4_Fig1.pdf` | GradCAM Visualization - Sample 1 |\n",
        "| Fig 16 | `RQ4/RQ4_Fig2.pdf` | GradCAM Visualization - Sample 2 |\n",
        "| Fig 17 | `RQ4/RQ4_Fig3.pdf` | GradCAM Visualization - Sample 3 |\n",
        "| Fig 18 | `RQ4/RQ4_Fig4.pdf` | SHAP Summary Plot |\n",
        "| Fig 19 | `RQ4/RQ4_Fig5.pdf` | SHAP Top 10 Features Bar Plot |\n",
        "\n",
        "#### RQ5 - Deployment Feasibility\n",
        "| Figure | Filename | Description |\n",
        "|--------|----------|-------------|\n",
        "| Fig 20 | `RQ5/RQ5_Fig1.pdf` | Inference Time Comparison |\n",
        "| Fig 21 | `RQ5/RQ5_Fig2.pdf` | Model Size Comparison |\n",
        "| Fig 22 | `RQ5/RQ5_Fig3.pdf` | Efficiency vs Accuracy Trade-off |\n",
        "| Fig 23 | `RQ5/RQ5_Fig4.pdf` | Calibration/Reliability Diagram |\n",
        "\n",
        "---\n",
        "\n",
        "### Tables (8 Excel Files)\n",
        "\n",
        "#### Methodology Section\n",
        "| Table | Filename | Description |\n",
        "|-------|----------|-------------|\n",
        "| Tab 1 | `network_configuration.xlsx` | Network configuration parameters |\n",
        "\n",
        "#### RQ1 - CNN Architecture Comparison\n",
        "| Table | Filename | Description |\n",
        "|-------|----------|-------------|\n",
        "| Tab 2 | `RQ1/RQ1_Tab1.xlsx` | CNN Comparison Results |\n",
        "\n",
        "#### RQ2 - Metalearner Ensemble\n",
        "| Table | Filename | Description |\n",
        "|-------|----------|-------------|\n",
        "| Tab 3 | `RQ2/RQ2_Tab1.xlsx` | Metalearner vs CNN Comparison |\n",
        "\n",
        "#### RQ3 - Hybrid Expert System\n",
        "| Table | Filename | Description |\n",
        "|-------|----------|-------------|\n",
        "| Tab 4 | `RQ3/RQ3_Tab1.xlsx` | Rule-based Integration Results |\n",
        "| Tab 5 | `RQ3/RQ3_Tab2.xlsx` | Ablation Study Results |\n",
        "\n",
        "#### RQ4 - Explainability\n",
        "| Table | Filename | Description |\n",
        "|-------|----------|-------------|\n",
        "| Tab 6 | `RQ4/RQ4_Tab1.xlsx` | Explainability Methods Summary |\n",
        "\n",
        "#### RQ5 - Deployment Feasibility\n",
        "| Table | Filename | Description |\n",
        "|-------|----------|-------------|\n",
        "| Tab 7 | `RQ5/RQ5_Tab1.xlsx` | Deployment Metrics |\n",
        "| Tab 8 | `RQ5/RQ5_Tab2.xlsx` | Final Results (all models) |\n",
        "\n",
        "---\n",
        "\n",
        "### Manual Creation Required\n",
        "| Type | Description |\n",
        "|------|-------------|\n",
        "| Figure | Methodology Workflow Diagram (create in PowerPoint) |\n",
        "| Table | Literature Review Summary Table (fill content in Excel) |\n",
        "|Comparison with Literature Table|Must fill content manually in Excel|\n",
        "\n",
        "---\n",
        "\n",
        "## Summary\n",
        "- **Total Figures:** 24 PDFs\n",
        "- **Total Tables:** 8 Excel files\n",
        "- **Manual Items:** 2 (workflow diagram + literature table)"
      ],
      "metadata": {
        "id": "TcL211z4uGGM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Project Steps**"
      ],
      "metadata": {
        "id": "3oOhWcWgRbUU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "1. Environment Setup & Library Configuration2. Dataset Acquisition & Initial Loading\n",
        "3. Data Preprocessing Pipeline & Augmentation Strategies\n",
        "4. Deep Learning Architecture Implementation\n",
        "5. Model Training & Optimization\n",
        "6. Baseline Performance Evaluation\n",
        "7. Multi-Scale Feature Extraction\n",
        "8. Ensemble Metalearner Development\n",
        "9. Knowledge-Based Rule Engine Design\n",
        "10. Hybrid Expert System Integration\n",
        "11. Systematic Ablation Analysis\n",
        "12. Comprehensive Results & Performance Visualization\n"
      ],
      "metadata": {
        "id": "y_7_9jbGtNpc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Environment Setup & Library Configuration"
      ],
      "metadata": {
        "id": "wYH1J1lHvDsy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.1 Project Header & Setup"
      ],
      "metadata": {
        "id": "LBUharY1vlcp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HL-bnqI8vBJf",
        "outputId": "0301b743-1a91-4099-b4ae-0f8a8ec09715"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            " Project structure created successfully\n",
            " Project root: /content/drive/MyDrive/Soil_Health_Classification_Phase2\n"
          ]
        }
      ],
      "source": [
        "# C1: Project Header & Setup\n",
        "\"\"\"\n",
        "PRECISION AGRICULTURE SOIL HEALTH CLASSIFICATION SYSTEM\n",
        "\n",
        "Course: Pattern Recognition (M.Sc.) - WS25\n",
        "Group Members:\n",
        "  - Student 1 (Technical Lead): Saba Sabzevari\n",
        "  - Student 1 (Figures & Presentation): Saba Sabzevari\n",
        "  - Student 2 (Report & Storytelling): Isha\n",
        "\n",
        "Project: Hybrid Expert System for Soil Type Classification\n",
        "Dataset: Kaggle Comprehensive Soil Classification Dataset\n",
        "Research Questions: 5 (RQ1-RQ5)\n",
        "\"\"\"\n",
        "\n",
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Set project paths\n",
        "import os\n",
        "PROJECT_ROOT = '/content/drive/MyDrive/Soil_Health_Classification_Phase2'\n",
        "os.makedirs(PROJECT_ROOT, exist_ok=True)\n",
        "os.makedirs(f'{PROJECT_ROOT}/checkpoints', exist_ok=True)\n",
        "os.makedirs(f'{PROJECT_ROOT}/figures', exist_ok=True)\n",
        "os.makedirs(f'{PROJECT_ROOT}/tables', exist_ok=True)\n",
        "\n",
        "# Create RQ folders\n",
        "for i in range(1, 6):\n",
        "    os.makedirs(f'{PROJECT_ROOT}/figures/RQ{i}', exist_ok=True)\n",
        "    os.makedirs(f'{PROJECT_ROOT}/tables/RQ{i}', exist_ok=True)\n",
        "\n",
        "print(\" Project structure created successfully\")\n",
        "print(f\" Project root: {PROJECT_ROOT}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.2 Import libraries"
      ],
      "metadata": {
        "id": "M2KZP4bov244"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# C2: Import libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
        "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report, roc_auc_score\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import xgboost as xgb\n",
        "import lightgbm as lgb\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.models as models\n",
        "from PIL import Image\n",
        "import cv2\n",
        "import pickle\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed(42)\n",
        "\n",
        "# Check GPU\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Device: {device}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SMPSiL_QvSzO",
        "outputId": "a38d8c75-3b31-4eea-abf5-e38797f0437f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n",
            "GPU: Tesla T4\n",
            "Memory: 15.83 GB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.3 Configuration for CPU optimization"
      ],
      "metadata": {
        "id": "f779uTGjwjIz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# C3: Configuration for CPU optimization\n",
        "CONFIG = {\n",
        "    'device': device,\n",
        "    'batch_size': 32,  # Smaller batch for CPU 16 --> 32\n",
        "    'num_workers': 2,  # CPU workers\n",
        "    'img_size': 224,  # Standard size\n",
        "    'epochs': 20,  # Reduced for CPU\n",
        "    'learning_rate': 1e-3,\n",
        "    'num_classes': 7,\n",
        "    'checkpoint_dir': f'{PROJECT_ROOT}/checkpoints',\n",
        "    'figures_dir': f'{PROJECT_ROOT}/figures',\n",
        "    'tables_dir': f'{PROJECT_ROOT}/tables',\n",
        "}\n",
        "\n",
        "# Soil classes\n",
        "SOIL_CLASSES = [\n",
        "    'Alluvial', 'Black', 'Clay', 'Laterite', 'Peat', 'Red', 'Yellow'\n",
        "]\n",
        "\n",
        "print(\"Configuration:\")\n",
        "for key, value in CONFIG.items():\n",
        "    print(f\"  {key}: {value}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H9aLlwWyv6Xu",
        "outputId": "21e5a863-bd4e-4e32-841d-f136b1cf956a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Configuration:\n",
            "  device: cuda\n",
            "  batch_size: 32\n",
            "  num_workers: 2\n",
            "  img_size: 224\n",
            "  epochs: 20\n",
            "  learning_rate: 0.001\n",
            "  num_classes: 7\n",
            "  checkpoint_dir: /content/drive/MyDrive/Soil_Health_Classification_Phase2/checkpoints\n",
            "  figures_dir: /content/drive/MyDrive/Soil_Health_Classification_Phase2/figures\n",
            "  tables_dir: /content/drive/MyDrive/Soil_Health_Classification_Phase2/tables\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Dataset Acquisition & Initial Loading"
      ],
      "metadata": {
        "id": "HHxmrJ5Rz0IB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.1 Extract dataset from existing zip file"
      ],
      "metadata": {
        "id": "0-uZregMwl7H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# C4: Extract and locate dataset\n",
        "import zipfile\n",
        "from pathlib import Path\n",
        "\n",
        "DATASET_DIR = f'{PROJECT_ROOT}/dataset'\n",
        "ZIP_FILE = f'{PROJECT_ROOT}/comprehensive-soil-classification-datasets.zip'\n",
        "\n",
        "if not os.path.exists(DATASET_DIR):\n",
        "    print(\"Extracting dataset...\")\n",
        "    with zipfile.ZipFile(ZIP_FILE, 'r') as zip_ref:\n",
        "        zip_ref.extractall(DATASET_DIR)\n",
        "    print(f\"Dataset extracted to: {DATASET_DIR}\")\n",
        "else:\n",
        "    print(f\"Dataset already exists at: {DATASET_DIR}\")\n",
        "\n",
        "# Use Original-Dataset (real images)\n",
        "ORIGINAL_DATA = f'{DATASET_DIR}/Orignal-Dataset'  # Note: \"Orignal\" spelling in dataset\n",
        "CYAUG_DATA = f'{DATASET_DIR}/CyAUG-Dataset'\n",
        "\n",
        "print(f\"\\nUsing: {ORIGINAL_DATA}\")\n",
        "print(f\"Available for augmentation: {CYAUG_DATA}\")\n",
        "\n",
        "# Check structure\n",
        "data_path = Path(ORIGINAL_DATA)\n",
        "soil_folders = sorted([d for d in data_path.iterdir() if d.is_dir()])\n",
        "print(f\"\\nFound {len(soil_folders)} soil type folders:\")\n",
        "for folder in soil_folders:\n",
        "    num_images = len(list(folder.glob('*.jpg'))) + len(list(folder.glob('*.png')))\n",
        "    print(f\"  {folder.name}: {num_images} images\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w3bS563ZwGNC",
        "outputId": "4e463c50-b895-42fb-efde-3fcbd67ad686"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting dataset...\n",
            "Dataset extracted to: /content/drive/MyDrive/Soil_Health_Classification_Phase2/dataset\n",
            "\n",
            "Using: /content/drive/MyDrive/Soil_Health_Classification_Phase2/dataset/Orignal-Dataset\n",
            "Available for augmentation: /content/drive/MyDrive/Soil_Health_Classification_Phase2/dataset/CyAUG-Dataset\n",
            "\n",
            "Found 7 soil type folders:\n",
            "  Alluvial_Soil: 50 images\n",
            "  Arid_Soil: 284 images\n",
            "  Black_Soil: 255 images\n",
            "  Laterite_Soil: 219 images\n",
            "  Mountain_Soil: 201 images\n",
            "  Red_Soil: 108 images\n",
            "  Yellow_Soil: 69 images\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Data Preprocessing Pipeline & Augmentation Strategies"
      ],
      "metadata": {
        "id": "u6-YrSSozuTr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.1 Update soil classes and prepare dataset\n"
      ],
      "metadata": {
        "id": "CE4IxHRPzd9B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# C5: Update soil classes and prepare dataset\n",
        "import glob\n",
        "from collections import Counter\n",
        "\n",
        "# Update actual soil classes from dataset\n",
        "SOIL_CLASSES = [\n",
        "    'Alluvial_Soil', 'Arid_Soil', 'Black_Soil', 'Laterite_Soil',\n",
        "    'Mountain_Soil', 'Red_Soil', 'Yellow_Soil'\n",
        "]\n",
        "\n",
        "# Create dataset index\n",
        "image_paths = []\n",
        "labels = []\n",
        "\n",
        "for idx, soil_class in enumerate(SOIL_CLASSES):\n",
        "    class_dir = f'{ORIGINAL_DATA}/{soil_class}'\n",
        "    images = glob.glob(f'{class_dir}/*.jpg') + glob.glob(f'{class_dir}/*.png')\n",
        "    image_paths.extend(images)\n",
        "    labels.extend([idx] * len(images))\n",
        "\n",
        "print(f\"Total images: {len(image_paths)}\")\n",
        "print(f\"\\nClass distribution:\")\n",
        "class_counts = Counter(labels)\n",
        "for idx, count in sorted(class_counts.items()):\n",
        "    print(f\"  {SOIL_CLASSES[idx]}: {count} ({count/len(labels)*100:.1f}%)\")\n",
        "\n",
        "# Split dataset: 70% train, 15% val, 15% test (stratified)\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(\n",
        "    image_paths, labels, test_size=0.3, stratify=labels, random_state=42\n",
        ")\n",
        "X_val, X_test, y_val, y_test = train_test_split(\n",
        "    X_temp, y_temp, test_size=0.5, stratify=y_temp, random_state=42\n",
        ")\n",
        "\n",
        "print(f\"\\nDataset splits:\")\n",
        "print(f\"  Train: {len(X_train)} images\")\n",
        "print(f\"  Val: {len(X_val)} images\")\n",
        "print(f\"  Test: {len(X_test)} images\")\n",
        "\n",
        "# Save splits for reproducibility\n",
        "splits = {\n",
        "    'train': {'paths': X_train, 'labels': y_train},\n",
        "    'val': {'paths': X_val, 'labels': y_val},\n",
        "    'test': {'paths': X_test, 'labels': y_test},\n",
        "    'classes': SOIL_CLASSES\n",
        "}\n",
        "\n",
        "with open(f'{CONFIG[\"checkpoint_dir\"]}/data_splits.pkl', 'wb') as f:\n",
        "    pickle.dump(splits, f)\n",
        "\n",
        "print(\"\\nData splits saved\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lg4V0i8rzK7o",
        "outputId": "2c6d8111-a3e9-4f04-8424-af9d01802120"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total images: 1186\n",
            "\n",
            "Class distribution:\n",
            "  Alluvial_Soil: 50 (4.2%)\n",
            "  Arid_Soil: 284 (23.9%)\n",
            "  Black_Soil: 255 (21.5%)\n",
            "  Laterite_Soil: 219 (18.5%)\n",
            "  Mountain_Soil: 201 (16.9%)\n",
            "  Red_Soil: 108 (9.1%)\n",
            "  Yellow_Soil: 69 (5.8%)\n",
            "\n",
            "Dataset splits:\n",
            "  Train: 830 images\n",
            "  Val: 178 images\n",
            "  Test: 178 images\n",
            "\n",
            "Data splits saved\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# C5b: Dataset Sample Visualization (for Methodology section)\n",
        "print(\"Creating dataset sample visualization...\")\n",
        "\n",
        "fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
        "axes = axes.flatten()\n",
        "\n",
        "for idx, soil_class in enumerate(SOIL_CLASSES[:7]):\n",
        "    class_dir = f'{ORIGINAL_DATA}/{soil_class}'\n",
        "    images = os.listdir(class_dir)\n",
        "    if images:\n",
        "        img_path = os.path.join(class_dir, images[0])\n",
        "        img = Image.open(img_path)\n",
        "        img = img.resize((224, 224))\n",
        "\n",
        "        axes[idx].imshow(img)\n",
        "        axes[idx].set_title(soil_class.replace('_', ' '), fontsize=12, fontweight='bold')\n",
        "        axes[idx].axis('off')\n",
        "\n",
        "axes[7].axis('off')\n",
        "\n",
        "plt.suptitle('Sample Images from Soil Classification Dataset', fontsize=16, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.savefig(f'{CONFIG[\"figures_dir\"]}/dataset_samples.pdf', dpi=300, bbox_inches='tight')\n",
        "plt.close()\n",
        "print(\"  Saved: dataset_samples.pdf\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_fzkCaS3oFU0",
        "outputId": "722a826f-fb5e-4eef-cfcd-6dc24485d456"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating dataset sample visualization...\n",
            "  Saved: dataset_samples.pdf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# C5c: Class Distribution Visualization (for Methodology section)\n",
        "print(\"Creating class distribution visualization...\")\n",
        "\n",
        "classes_short = [SOIL_CLASSES[i].replace('_Soil', '') for i in range(len(SOIL_CLASSES))]\n",
        "counts = [class_counts[i] for i in range(len(SOIL_CLASSES))]\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(12, 6))\n",
        "\n",
        "patterns = ['///', '...', '\\\\\\\\\\\\', '|||', 'xxx', '+++', 'ooo']\n",
        "colors = ['#E0E0E0', '#C0C0C0', '#A0A0A0', '#808080', '#606060', '#404040', '#202020']\n",
        "\n",
        "bars = ax.bar(range(len(classes_short)), counts, color=colors, edgecolor='black', linewidth=1.5)\n",
        "\n",
        "for bar, pattern in zip(bars, patterns):\n",
        "    bar.set_hatch(pattern)\n",
        "\n",
        "for bar, count in zip(bars, counts):\n",
        "    ax.text(bar.get_x() + bar.get_width()/2., bar.get_height() + 5,\n",
        "            f'{count}', ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
        "\n",
        "ax.set_xticks(range(len(classes_short)))\n",
        "ax.set_xticklabels(classes_short, rotation=45, ha='right', fontsize=11)\n",
        "ax.set_xlabel('Soil Type', fontsize=12)\n",
        "ax.set_ylabel('Number of Samples', fontsize=12)\n",
        "ax.set_title('Class Distribution in Soil Classification Dataset', fontsize=14, fontweight='bold')\n",
        "ax.grid(axis='y', alpha=0.3)\n",
        "\n",
        "ax.text(0.98, 0.98, f'Total: {len(labels)}\\nTrain: {len(X_train)} (70%)\\nVal: {len(X_val)} (15%)\\nTest: {len(X_test)} (15%)',\n",
        "        transform=ax.transAxes, fontsize=10, verticalalignment='top', horizontalalignment='right',\n",
        "        bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(f'{CONFIG[\"figures_dir\"]}/dataset_distribution.pdf', dpi=300, bbox_inches='tight')\n",
        "plt.close()\n",
        "print(\"  Saved: dataset_distribution.pdf\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iySQnWq3oTF3",
        "outputId": "9cd60e09-ed11-413b-98dd-656b38e85f6f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating class distribution visualization...\n",
            "  Saved: dataset_distribution.pdf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.2 PyTorch dataset class with augmentation"
      ],
      "metadata": {
        "id": "17K3ixwN2T9o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# C6: PyTorch dataset class with augmentation\n",
        "class SoilDataset(Dataset):\n",
        "    def __init__(self, image_paths, labels, transform=None):\n",
        "        self.image_paths = image_paths\n",
        "        self.labels = labels\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.image_paths[idx]\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, label\n",
        "\n",
        "# Data transforms\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize((CONFIG['img_size'], CONFIG['img_size'])),\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.RandomVerticalFlip(p=0.3),\n",
        "    transforms.RandomRotation(15),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "val_transform = transforms.Compose([\n",
        "    transforms.Resize((CONFIG['img_size'], CONFIG['img_size'])),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Create datasets\n",
        "train_dataset = SoilDataset(X_train, y_train, train_transform)\n",
        "val_dataset = SoilDataset(X_val, y_val, val_transform)\n",
        "test_dataset = SoilDataset(X_test, y_test, val_transform)\n",
        "\n",
        "# Create dataloaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=CONFIG['batch_size'],\n",
        "                          shuffle=True, num_workers=CONFIG['num_workers'])\n",
        "val_loader = DataLoader(val_dataset, batch_size=CONFIG['batch_size'],\n",
        "                        shuffle=False, num_workers=CONFIG['num_workers'])\n",
        "test_loader = DataLoader(test_dataset, batch_size=CONFIG['batch_size'],\n",
        "                         shuffle=False, num_workers=CONFIG['num_workers'])\n",
        "\n",
        "print(f\"Dataloaders created:\")\n",
        "print(f\"  Train batches: {len(train_loader)}\")\n",
        "print(f\"  Val batches: {len(val_loader)}\")\n",
        "print(f\"  Test batches: {len(test_loader)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9hzIO4Wv2Idr",
        "outputId": "02e8095a-c879-44ef-e451-7a53dc33bf7e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataloaders created:\n",
            "  Train batches: 26\n",
            "  Val batches: 6\n",
            "  Test batches: 6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Deep Learning Architecture Implementation"
      ],
      "metadata": {
        "id": "1tIO_HKa2pj1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4.1 CNN architectures for RQ1 comparison"
      ],
      "metadata": {
        "id": "LYSKKrZGwf3l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# C7: CNN architectures for RQ1 comparison\n",
        "def get_model(model_name, num_classes=7, pretrained=True):\n",
        "    \"\"\"Load pretrained CNN and modify for soil classification\"\"\"\n",
        "\n",
        "    if model_name == 'resnet50':\n",
        "        model = models.resnet50(pretrained=pretrained)\n",
        "        model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
        "        feature_dim = 2048\n",
        "\n",
        "    elif model_name == 'efficientnet_b3':\n",
        "        model = models.efficientnet_b3(pretrained=pretrained)\n",
        "        model.classifier[1] = nn.Linear(model.classifier[1].in_features, num_classes)\n",
        "        feature_dim = 1536\n",
        "\n",
        "    elif model_name == 'vit_b_16':\n",
        "        model = models.vit_b_16(pretrained=pretrained)\n",
        "        model.heads.head = nn.Linear(model.heads.head.in_features, num_classes)\n",
        "        feature_dim = 768\n",
        "\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown model: {model_name}\")\n",
        "\n",
        "    return model, feature_dim\n",
        "\n",
        "# Test model loading\n",
        "print(\"Testing CNN architectures for RQ1:\")\n",
        "for model_name in ['resnet50', 'efficientnet_b3', 'vit_b_16']:\n",
        "    model, feat_dim = get_model(model_name, CONFIG['num_classes'])\n",
        "    total_params = sum(p.numel() for p in model.parameters())\n",
        "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    print(f\"\\n{model_name}:\")\n",
        "    print(f\"  Feature dimension: {feat_dim}\")\n",
        "    print(f\"  Total parameters: {total_params:,}\")\n",
        "    print(f\"  Trainable parameters: {trainable_params:,}\")\n",
        "    del model  # Free memory"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xdk52_Jd2X1Y",
        "outputId": "8ea570cd-ae72-47bb-c785-6d42ef217136"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing CNN architectures for RQ1:\n",
            "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 97.8M/97.8M [00:00<00:00, 183MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "resnet50:\n",
            "  Feature dimension: 2048\n",
            "  Total parameters: 23,522,375\n",
            "  Trainable parameters: 23,522,375\n",
            "Downloading: \"https://download.pytorch.org/models/efficientnet_b3_rwightman-b3899882.pth\" to /root/.cache/torch/hub/checkpoints/efficientnet_b3_rwightman-b3899882.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 47.2M/47.2M [00:00<00:00, 160MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "efficientnet_b3:\n",
            "  Feature dimension: 1536\n",
            "  Total parameters: 10,706,991\n",
            "  Trainable parameters: 10,706,991\n",
            "Downloading: \"https://download.pytorch.org/models/vit_b_16-c867db91.pth\" to /root/.cache/torch/hub/checkpoints/vit_b_16-c867db91.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 330M/330M [00:02<00:00, 138MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "vit_b_16:\n",
            "  Feature dimension: 768\n",
            "  Total parameters: 85,804,039\n",
            "  Trainable parameters: 85,804,039\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Model Training & Optimization"
      ],
      "metadata": {
        "id": "OcekDFui3ePd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5.1 Training function with checkpointing"
      ],
      "metadata": {
        "id": "u6Ak6QTf3hMf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# C8: Training function with checkpointing\n",
        "def train_cnn(model, train_loader, val_loader, model_name, epochs=20):\n",
        "    \"\"\"Train CNN with early stopping and checkpointing\"\"\"\n",
        "\n",
        "    model = model.to(device)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=CONFIG['learning_rate'], weight_decay=0.01)\n",
        "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', patience=3, factor=0.5)\n",
        "\n",
        "    best_val_acc = 0\n",
        "    patience_counter = 0\n",
        "    patience = 5\n",
        "\n",
        "    history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # Training\n",
        "        model.train()\n",
        "        train_loss = 0\n",
        "        train_correct = 0\n",
        "        train_total = 0\n",
        "\n",
        "        pbar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{epochs}')\n",
        "        for images, labels in pbar:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            train_total += labels.size(0)\n",
        "            train_correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "            pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
        "\n",
        "        train_acc = 100. * train_correct / train_total\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        val_loss = 0\n",
        "        val_correct = 0\n",
        "        val_total = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for images, labels in val_loader:\n",
        "                images, labels = images.to(device), labels.to(device)\n",
        "                outputs = model(images)\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "                val_loss += loss.item()\n",
        "                _, predicted = outputs.max(1)\n",
        "                val_total += labels.size(0)\n",
        "                val_correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "        val_acc = 100. * val_correct / val_total\n",
        "\n",
        "        # Update history\n",
        "        history['train_loss'].append(train_loss / len(train_loader))\n",
        "        history['train_acc'].append(train_acc)\n",
        "        history['val_loss'].append(val_loss / len(val_loader))\n",
        "        history['val_acc'].append(val_acc)\n",
        "\n",
        "        print(f'Epoch {epoch+1}: Train Acc: {train_acc:.2f}% | Val Acc: {val_acc:.2f}%')\n",
        "\n",
        "        # Scheduler step\n",
        "        scheduler.step(val_acc)\n",
        "\n",
        "        # Save best model\n",
        "        if val_acc > best_val_acc:\n",
        "            best_val_acc = val_acc\n",
        "            patience_counter = 0\n",
        "            checkpoint = {\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'val_acc': val_acc,\n",
        "                'epoch': epoch,\n",
        "                'history': history\n",
        "            }\n",
        "            torch.save(checkpoint, f'{CONFIG[\"checkpoint_dir\"]}/{model_name}_best.pth')\n",
        "            print(f'  Saved best model (val_acc: {val_acc:.2f}%)')\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            if patience_counter >= patience:\n",
        "                print(f'Early stopping at epoch {epoch+1}')\n",
        "                break\n",
        "\n",
        "    return history, best_val_acc\n",
        "\n",
        "print(\"Training function ready with checkpointing and early stopping\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2IsleigX2rVS",
        "outputId": "8f2d4b72-fe70-404d-fe21-9588f1776e89"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training function ready with checkpointing and early stopping\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5.2 Train ResNet50 (RQ1) or load from checkpoint"
      ],
      "metadata": {
        "id": "-LKGvqBM3sSv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# C9: Train ResNet50 (RQ1) - CPU optimized with minimal epochs\n",
        "model_name = 'resnet50'\n",
        "checkpoint_path = f'{CONFIG[\"checkpoint_dir\"]}/{model_name}_best.pth'\n",
        "\n",
        "# Minimal epochs for CPU\n",
        "CONFIG['epochs'] = 10  # 5 epochs instead of 10\n",
        "\n",
        "if os.path.exists(checkpoint_path):\n",
        "    print(f\"Loading {model_name} from checkpoint...\")\n",
        "    model, feature_dim = get_model(model_name, CONFIG['num_classes'])\n",
        "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    resnet_history = checkpoint['history']\n",
        "    resnet_val_acc = checkpoint['val_acc']\n",
        "    print(f\"Loaded model with val_acc: {resnet_val_acc:.2f}%\")\n",
        "else:\n",
        "    print(f\"Training {model_name} (GPU mode - 10 epochs)...\")\n",
        "    model, feature_dim = get_model(model_name, CONFIG['num_classes'], pretrained=True)\n",
        "    resnet_history, resnet_val_acc = train_cnn(\n",
        "        model, train_loader, val_loader, model_name, epochs=CONFIG['epochs']\n",
        "    )\n",
        "    print(f\"Training complete! Best val_acc: {resnet_val_acc:.2f}%\")\n",
        "\n",
        "resnet_model = model\n",
        "resnet_feature_dim = feature_dim\n",
        "print(f\"ResNet50 ready (feature_dim: {feature_dim})\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fR_SXH7T4_Da",
        "outputId": "2258d384-5015-4c71-ca14-efe79194ffef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training resnet50 (GPU mode - 10 epochs)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/10: 100%|██████████| 26/26 [00:14<00:00,  1.74it/s, loss=0.9833]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: Train Acc: 65.30% | Val Acc: 39.33%\n",
            "  Saved best model (val_acc: 39.33%)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/10: 100%|██████████| 26/26 [00:13<00:00,  1.94it/s, loss=0.7750]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2: Train Acc: 70.96% | Val Acc: 75.84%\n",
            "  Saved best model (val_acc: 75.84%)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/10: 100%|██████████| 26/26 [00:13<00:00,  1.88it/s, loss=0.7769]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3: Train Acc: 73.73% | Val Acc: 79.78%\n",
            "  Saved best model (val_acc: 79.78%)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4/10: 100%|██████████| 26/26 [00:17<00:00,  1.47it/s, loss=0.5624]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4: Train Acc: 78.67% | Val Acc: 69.10%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5/10: 100%|██████████| 26/26 [00:13<00:00,  1.92it/s, loss=0.5579]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5: Train Acc: 81.08% | Val Acc: 81.46%\n",
            "  Saved best model (val_acc: 81.46%)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6/10: 100%|██████████| 26/26 [00:13<00:00,  1.95it/s, loss=0.4207]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6: Train Acc: 82.53% | Val Acc: 71.35%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7/10: 100%|██████████| 26/26 [00:13<00:00,  1.87it/s, loss=0.5261]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7: Train Acc: 78.07% | Val Acc: 79.21%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8/10: 100%|██████████| 26/26 [00:13<00:00,  1.86it/s, loss=0.5064]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8: Train Acc: 83.86% | Val Acc: 75.84%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9/10: 100%|██████████| 26/26 [00:15<00:00,  1.71it/s, loss=0.6639]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9: Train Acc: 82.77% | Val Acc: 82.58%\n",
            "  Saved best model (val_acc: 82.58%)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 10/10: 100%|██████████| 26/26 [00:14<00:00,  1.83it/s, loss=0.6967]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10: Train Acc: 86.02% | Val Acc: 81.46%\n",
            "Training complete! Best val_acc: 82.58%\n",
            "ResNet50 ready (feature_dim: 2048)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5.3 Train EfficientNet-B3 (RQ1) or load from checkpoint"
      ],
      "metadata": {
        "id": "OiyZiaeTA9QS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# C10: Train EfficientNet-B3 (RQ1) or load from checkpoint\n",
        "model_name = 'efficientnet_b3'\n",
        "checkpoint_path = f'{CONFIG[\"checkpoint_dir\"]}/{model_name}_best.pth'\n",
        "\n",
        "if os.path.exists(checkpoint_path):\n",
        "    print(f\"Loading {model_name} from checkpoint...\")\n",
        "    model, feature_dim = get_model(model_name, CONFIG['num_classes'])\n",
        "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    efficientnet_history = checkpoint['history']\n",
        "    efficientnet_val_acc = checkpoint['val_acc']\n",
        "    print(f\"Loaded model with val_acc: {efficientnet_val_acc:.2f}%\")\n",
        "else:\n",
        "    print(f\"Training {model_name} (GPU mode - 10 epochs)...\")\n",
        "    model, feature_dim = get_model(model_name, CONFIG['num_classes'], pretrained=True)\n",
        "    efficientnet_history, efficientnet_val_acc = train_cnn(\n",
        "        model, train_loader, val_loader, model_name, epochs=CONFIG['epochs']\n",
        "    )\n",
        "    print(f\"Training complete! Best val_acc: {efficientnet_val_acc:.2f}%\")\n",
        "\n",
        "efficientnet_model = model\n",
        "efficientnet_feature_dim = feature_dim\n",
        "print(f\"EfficientNet-B3 ready (feature_dim: {feature_dim})\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DQ_1atC25IAG",
        "outputId": "9aa03267-7f00-4089-c86a-25e8c72b1ea8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training efficientnet_b3 (GPU mode - 10 epochs)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/10: 100%|██████████| 26/26 [00:13<00:00,  1.87it/s, loss=0.8721]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: Train Acc: 72.05% | Val Acc: 85.39%\n",
            "  Saved best model (val_acc: 85.39%)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/10: 100%|██████████| 26/26 [00:14<00:00,  1.80it/s, loss=0.4381]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2: Train Acc: 85.30% | Val Acc: 87.64%\n",
            "  Saved best model (val_acc: 87.64%)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/10: 100%|██████████| 26/26 [00:14<00:00,  1.77it/s, loss=0.3741]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3: Train Acc: 90.24% | Val Acc: 88.20%\n",
            "  Saved best model (val_acc: 88.20%)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4/10: 100%|██████████| 26/26 [00:13<00:00,  1.86it/s, loss=0.2992]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4: Train Acc: 92.17% | Val Acc: 88.76%\n",
            "  Saved best model (val_acc: 88.76%)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5/10: 100%|██████████| 26/26 [00:14<00:00,  1.80it/s, loss=1.2143]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5: Train Acc: 92.65% | Val Acc: 88.76%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6/10: 100%|██████████| 26/26 [00:14<00:00,  1.79it/s, loss=0.3920]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6: Train Acc: 90.12% | Val Acc: 84.83%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7/10: 100%|██████████| 26/26 [00:13<00:00,  1.90it/s, loss=0.1269]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7: Train Acc: 93.37% | Val Acc: 90.45%\n",
            "  Saved best model (val_acc: 90.45%)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8/10: 100%|██████████| 26/26 [00:18<00:00,  1.44it/s, loss=0.1687]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8: Train Acc: 95.42% | Val Acc: 89.89%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9/10: 100%|██████████| 26/26 [00:13<00:00,  1.90it/s, loss=0.1667]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9: Train Acc: 94.94% | Val Acc: 90.45%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 10/10: 100%|██████████| 26/26 [00:13<00:00,  1.90it/s, loss=0.2335]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10: Train Acc: 94.82% | Val Acc: 89.89%\n",
            "Training complete! Best val_acc: 90.45%\n",
            "EfficientNet-B3 ready (feature_dim: 1536)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5.4 Train Vision Transformer (RQ1) or load from checkpoint"
      ],
      "metadata": {
        "id": "UyzZmYLhGYzj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# C11: Train Vision Transformer (RQ1) or load from checkpoint\n",
        "model_name = 'vit_b_16'\n",
        "checkpoint_path = f'{CONFIG[\"checkpoint_dir\"]}/{model_name}_best.pth'\n",
        "\n",
        "if os.path.exists(checkpoint_path):\n",
        "    print(f\"Loading {model_name} from checkpoint...\")\n",
        "    model, feature_dim = get_model(model_name, CONFIG['num_classes'])\n",
        "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    vit_history = checkpoint['history']\n",
        "    vit_val_acc = checkpoint['val_acc']\n",
        "    print(f\"Loaded model with val_acc: {vit_val_acc:.2f}%\")\n",
        "else:\n",
        "    print(f\"Training {model_name} (GPU mode - 10 epochs)...\")\n",
        "    model, feature_dim = get_model(model_name, CONFIG['num_classes'], pretrained=True)\n",
        "    vit_history, vit_val_acc = train_cnn(\n",
        "        model, train_loader, val_loader, model_name, epochs=CONFIG['epochs']\n",
        "    )\n",
        "    print(f\"Training complete! Best val_acc: {vit_val_acc:.2f}%\")\n",
        "\n",
        "vit_model = model\n",
        "vit_feature_dim = feature_dim\n",
        "print(f\"Vision Transformer ready (feature_dim: {feature_dim})\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SoQMFVbMA_eF",
        "outputId": "3bf2fbe8-e426-45ff-e06f-bd400df393fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training vit_b_16 (GPU mode - 10 epochs)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/10: 100%|██████████| 26/26 [00:30<00:00,  1.18s/it, loss=1.1730]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: Train Acc: 34.82% | Val Acc: 47.19%\n",
            "  Saved best model (val_acc: 47.19%)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/10: 100%|██████████| 26/26 [00:30<00:00,  1.18s/it, loss=1.3955]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2: Train Acc: 53.25% | Val Acc: 51.69%\n",
            "  Saved best model (val_acc: 51.69%)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/10: 100%|██████████| 26/26 [00:50<00:00,  1.95s/it, loss=1.2367]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3: Train Acc: 55.54% | Val Acc: 65.73%\n",
            "  Saved best model (val_acc: 65.73%)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4/10: 100%|██████████| 26/26 [00:50<00:00,  1.95s/it, loss=0.6830]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4: Train Acc: 56.02% | Val Acc: 67.98%\n",
            "  Saved best model (val_acc: 67.98%)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5/10: 100%|██████████| 26/26 [00:50<00:00,  1.96s/it, loss=0.7470]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5: Train Acc: 63.13% | Val Acc: 60.11%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6/10: 100%|██████████| 26/26 [00:35<00:00,  1.38s/it, loss=0.6209]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6: Train Acc: 63.25% | Val Acc: 74.72%\n",
            "  Saved best model (val_acc: 74.72%)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7/10: 100%|██████████| 26/26 [00:50<00:00,  1.93s/it, loss=0.7590]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7: Train Acc: 65.30% | Val Acc: 69.10%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8/10: 100%|██████████| 26/26 [00:30<00:00,  1.17s/it, loss=0.7962]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8: Train Acc: 68.31% | Val Acc: 71.35%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9/10: 100%|██████████| 26/26 [00:29<00:00,  1.15s/it, loss=0.7883]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9: Train Acc: 70.96% | Val Acc: 74.16%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 10/10: 100%|██████████| 26/26 [00:31<00:00,  1.20s/it, loss=0.7360]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10: Train Acc: 72.41% | Val Acc: 70.79%\n",
            "Training complete! Best val_acc: 74.72%\n",
            "Vision Transformer ready (feature_dim: 768)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Baseline Performance Evaluation"
      ],
      "metadata": {
        "id": "C9O_ZOi-PGdG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6.1 RQ1 results - CNN architecture comparison\n",
        "\n"
      ],
      "metadata": {
        "id": "jaqDNnDwOW9T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# C12: RQ1 results - CNN architecture comparison\n",
        "rq1_results = {\n",
        "    'ResNet50': resnet_val_acc,\n",
        "    'EfficientNet-B3': efficientnet_val_acc,\n",
        "    'Vision Transformer': vit_val_acc\n",
        "}\n",
        "\n",
        "print(\"RQ1: CNN Architecture Comparison Results\")\n",
        "#print(\"=\" * 50)\n",
        "for model_name, val_acc in rq1_results.items():\n",
        "    print(f\"{model_name:20s}: {val_acc:.2f}%\")\n",
        "\n",
        "# Select best CNN for next stages\n",
        "best_cnn_name = max(rq1_results, key=rq1_results.get)\n",
        "best_cnn_acc = rq1_results[best_cnn_name]\n",
        "\n",
        "if best_cnn_name == 'ResNet50':\n",
        "    best_model = resnet_model\n",
        "    best_feature_dim = resnet_feature_dim\n",
        "elif best_cnn_name == 'EfficientNet-B3':\n",
        "    best_model = efficientnet_model\n",
        "    best_feature_dim = efficientnet_feature_dim\n",
        "else:\n",
        "    best_model = vit_model\n",
        "    best_feature_dim = vit_feature_dim\n",
        "\n",
        "print(f\"\\nBest CNN: {best_cnn_name} ({best_cnn_acc:.2f}%)\")\n",
        "print(f\"Feature dimension: {best_feature_dim}\")\n",
        "\n",
        "# Save RQ1 table\n",
        "rq1_df = pd.DataFrame({\n",
        "    'Model': list(rq1_results.keys()),\n",
        "    'Validation Accuracy (%)': [f\"{v:.2f}\" for v in rq1_results.values()]\n",
        "})\n",
        "rq1_df.to_excel(f'{CONFIG[\"tables_dir\"]}/RQ1/RQ1_Tab1.xlsx', index=False)\n",
        "print(\"\\nRQ1 table saved!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6czwSiQsGc3c",
        "outputId": "6dd3b384-fae0-4311-ce07-4c5ba396c2fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RQ1: CNN Architecture Comparison Results\n",
            "ResNet50            : 82.58%\n",
            "EfficientNet-B3     : 90.45%\n",
            "Vision Transformer  : 74.72%\n",
            "\n",
            "Best CNN: EfficientNet-B3 (90.45%)\n",
            "Feature dimension: 1536\n",
            "\n",
            "RQ1 table saved!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# C12b: Network Configuration Table (for Methodology section)\n",
        "print(\"Creating network configuration table...\")\n",
        "\n",
        "config_data = {\n",
        "    'Parameter': [\n",
        "        'Input Image Size',\n",
        "        'Batch Size',\n",
        "        'Number of Epochs',\n",
        "        'Learning Rate',\n",
        "        'Optimizer',\n",
        "        'Weight Decay',\n",
        "        'Scheduler',\n",
        "        'Early Stopping Patience',\n",
        "        'Training Samples',\n",
        "        'Validation Samples',\n",
        "        'Test Samples',\n",
        "        'Number of Classes'\n",
        "    ],\n",
        "    'Value': [\n",
        "        f'{CONFIG[\"img_size\"]} × {CONFIG[\"img_size\"]}',\n",
        "        str(CONFIG['batch_size']),\n",
        "        str(CONFIG['epochs']),\n",
        "        str(CONFIG['learning_rate']),\n",
        "        'AdamW',\n",
        "        '0.01',\n",
        "        'ReduceLROnPlateau (factor=0.5, patience=3)',\n",
        "        '5 epochs',\n",
        "        str(len(X_train)),\n",
        "        str(len(X_val)),\n",
        "        str(len(X_test)),\n",
        "        str(CONFIG['num_classes'])\n",
        "    ]\n",
        "}\n",
        "\n",
        "config_df = pd.DataFrame(config_data)\n",
        "config_df.to_excel(f'{CONFIG[\"tables_dir\"]}/network_configuration.xlsx', index=False)\n",
        "print(\"  Saved: network_configuration.xlsx\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ys0K_y6nonpj",
        "outputId": "d08ee695-eb0b-4f45-e93c-e1098b982c36"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating network configuration table...\n",
            "  Saved: network_configuration.xlsx\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. Multi-Scale Feature Extraction"
      ],
      "metadata": {
        "id": "ZHEblGTzPKaT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7.1 Extract CNN features for metalearner (RQ2)"
      ],
      "metadata": {
        "id": "D6r-AgLEPMfs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# C13: Extract CNN features for metalearner (RQ2)\n",
        "def extract_features(model, dataloader, feature_dim):\n",
        "    \"\"\"Extract CNN features for metalearning\"\"\"\n",
        "    model.eval()\n",
        "    model = model.to(device)\n",
        "\n",
        "    features_list = []\n",
        "    labels_list = []\n",
        "    predictions_list = []\n",
        "\n",
        "    # Remove final classification layer to get features\n",
        "    if hasattr(model, 'fc'):  # ResNet\n",
        "        feature_extractor = nn.Sequential(*list(model.children())[:-1])\n",
        "    elif hasattr(model, 'classifier'):  # EfficientNet\n",
        "        feature_extractor = nn.Sequential(*list(model.children())[:-1])\n",
        "    else:  # ViT\n",
        "        feature_extractor = model\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in tqdm(dataloader, desc='Extracting features'):\n",
        "            images = images.to(device)\n",
        "\n",
        "            if hasattr(model, 'fc'):  # ResNet\n",
        "                feats = feature_extractor(images)\n",
        "                feats = feats.view(feats.size(0), -1)\n",
        "            elif hasattr(model, 'classifier'):  # EfficientNet\n",
        "                feats = feature_extractor(images)\n",
        "                feats = torch.nn.functional.adaptive_avg_pool2d(feats, 1)\n",
        "                feats = feats.view(feats.size(0), -1)\n",
        "            else:  # ViT\n",
        "                feats = feature_extractor(images)\n",
        "                feats = feats[:, 0]  # CLS token\n",
        "\n",
        "            # Get predictions\n",
        "            outputs = model(images)\n",
        "            _, preds = outputs.max(1)\n",
        "\n",
        "            features_list.append(feats.cpu().numpy())\n",
        "            labels_list.append(labels.numpy())\n",
        "            predictions_list.append(preds.cpu().numpy())\n",
        "\n",
        "    features = np.vstack(features_list)\n",
        "    labels = np.concatenate(labels_list)\n",
        "    predictions = np.concatenate(predictions_list)\n",
        "\n",
        "    return features, labels, predictions\n",
        "\n",
        "# Extract features from best CNN (EfficientNet-B3)\n",
        "print(\"Extracting features from EfficientNet-B3...\")\n",
        "train_features, train_labels, train_preds = extract_features(best_model, train_loader, best_feature_dim)\n",
        "val_features, val_labels, val_preds = extract_features(best_model, val_loader, best_feature_dim)\n",
        "test_features, test_labels, test_preds = extract_features(best_model, test_loader, best_feature_dim)\n",
        "\n",
        "print(f\"\\nFeature shapes:\")\n",
        "print(f\"  Train: {train_features.shape}\")\n",
        "print(f\"  Val: {val_features.shape}\")\n",
        "print(f\"  Test: {test_features.shape}\")\n",
        "\n",
        "# Save features\n",
        "np.savez(f'{CONFIG[\"checkpoint_dir\"]}/cnn_features.npz',\n",
        "         train_features=train_features, train_labels=train_labels,\n",
        "         val_features=val_features, val_labels=val_labels,\n",
        "         test_features=test_features, test_labels=test_labels)\n",
        "\n",
        "print(\"\\nFeatures saved\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qrXgmA3pObcH",
        "outputId": "2dbb40c0-0a3f-469c-fa0f-26f65cbbe4fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting features from EfficientNet-B3...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting features: 100%|██████████| 26/26 [00:11<00:00,  2.18it/s]\n",
            "Extracting features: 100%|██████████| 6/6 [00:02<00:00,  2.45it/s]\n",
            "Extracting features: 100%|██████████| 6/6 [00:02<00:00,  2.35it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Feature shapes:\n",
            "  Train: (830, 1536)\n",
            "  Val: (178, 1536)\n",
            "  Test: (178, 1536)\n",
            "\n",
            "Features saved\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. Ensemble Metalearner Development"
      ],
      "metadata": {
        "id": "X9OoW4qjSAYg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "8.1 Train metalearner ensemble (RQ2)"
      ],
      "metadata": {
        "id": "xfC_iB0sSCKY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# C14: Train metalearner ensemble (RQ2)\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "\n",
        "# Base learners for stacking\n",
        "print(\"Training metalearner base models\")\n",
        "\n",
        "# Random Forest\n",
        "print(\"  Training Random Forest\")\n",
        "rf = RandomForestClassifier(n_estimators=100, max_depth=15, random_state=42, n_jobs=-1)\n",
        "rf.fit(train_features, train_labels)\n",
        "rf_val_acc = accuracy_score(val_labels, rf.predict(val_features))\n",
        "print(f\"    Val Acc: {rf_val_acc*100:.2f}%\")\n",
        "\n",
        "# XGBoost\n",
        "print(\"  Training XGBoost...\")\n",
        "xgb_model = xgb.XGBClassifier(n_estimators=100, max_depth=7, learning_rate=0.1,\n",
        "                               random_state=42, n_jobs=-1)\n",
        "xgb_model.fit(train_features, train_labels)\n",
        "xgb_val_acc = accuracy_score(val_labels, xgb_model.predict(val_features))\n",
        "print(f\"    Val Acc: {xgb_val_acc*100:.2f}%\")\n",
        "\n",
        "# LightGBM\n",
        "print(\"  Training LightGBM...\")\n",
        "lgb_model = lgb.LGBMClassifier(n_estimators=100, max_depth=7, learning_rate=0.1,\n",
        "                                random_state=42, n_jobs=-1, verbose=-1)\n",
        "lgb_model.fit(train_features, train_labels)\n",
        "lgb_val_acc = accuracy_score(val_labels, lgb_model.predict(val_features))\n",
        "print(f\"    Val Acc: {lgb_val_acc*100:.2f}%\")\n",
        "\n",
        "# SVM\n",
        "print(\"  Training SVM\")\n",
        "svm = SVC(kernel='rbf', probability=True, random_state=42)\n",
        "svm.fit(train_features, train_labels)\n",
        "svm_val_acc = accuracy_score(val_labels, svm.predict(val_features))\n",
        "print(f\"    Val Acc: {svm_val_acc*100:.2f}%\")\n",
        "\n",
        "# Meta-learner: Logistic Regression on base predictions\n",
        "print(\"\\nTraining meta-learner (Logistic Regression)\")\n",
        "train_meta_features = np.column_stack([\n",
        "    rf.predict_proba(train_features),\n",
        "    xgb_model.predict_proba(train_features),\n",
        "    lgb_model.predict_proba(train_features),\n",
        "    svm.predict_proba(train_features)\n",
        "])\n",
        "\n",
        "val_meta_features = np.column_stack([\n",
        "    rf.predict_proba(val_features),\n",
        "    xgb_model.predict_proba(val_features),\n",
        "    lgb_model.predict_proba(val_features),\n",
        "    svm.predict_proba(val_features)\n",
        "])\n",
        "\n",
        "meta_learner = LogisticRegression(max_iter=1000, random_state=42)\n",
        "meta_learner.fit(train_meta_features, train_labels)\n",
        "meta_val_acc = accuracy_score(val_labels, meta_learner.predict(val_meta_features))\n",
        "\n",
        "print(f\"\\nMetalearner Val Acc: {meta_val_acc*100:.2f}%\")\n",
        "\n",
        "# Save models\n",
        "with open(f'{CONFIG[\"checkpoint_dir\"]}/metalearner.pkl', 'wb') as f:\n",
        "    pickle.dump({\n",
        "        'rf': rf, 'xgb': xgb_model, 'lgb': lgb_model, 'svm': svm,\n",
        "        'meta_learner': meta_learner\n",
        "    }, f)\n",
        "\n",
        "print(\"Metalearner models saved\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wMzRH_WMPPr7",
        "outputId": "3477f8ac-fb0c-478c-c9a3-aa8d6813cd54"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training metalearner base models\n",
            "  Training Random Forest\n",
            "    Val Acc: 91.57%\n",
            "  Training XGBoost...\n",
            "    Val Acc: 89.89%\n",
            "  Training LightGBM...\n",
            "    Val Acc: 90.45%\n",
            "  Training SVM\n",
            "    Val Acc: 91.01%\n",
            "\n",
            "Training meta-learner (Logistic Regression)\n",
            "\n",
            "Metalearner Val Acc: 89.89%\n",
            "Metalearner models saved\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "8.2 RQ2 results - CNN vs Metalearner comparison"
      ],
      "metadata": {
        "id": "8rYJgyBPWARk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# C15: RQ2 results - CNN vs Metalearner comparison\n",
        "rq2_results = {\n",
        "    'CNN Only (EfficientNet-B3)': best_cnn_acc,\n",
        "    'Random Forest': rf_val_acc * 100,\n",
        "    'XGBoost': xgb_val_acc * 100,\n",
        "    'LightGBM': lgb_val_acc * 100,\n",
        "    'SVM': svm_val_acc * 100,\n",
        "    'Metalearner (Stacking)': meta_val_acc * 100\n",
        "}\n",
        "\n",
        "print(\"RQ2: Metalearner vs CNN Comparison\")\n",
        "print(\"=\" * 50)\n",
        "for model_name, val_acc in rq2_results.items():\n",
        "    print(f\"{model_name:30s}: {val_acc:.2f}%\")\n",
        "\n",
        "improvement = meta_val_acc * 100 - best_cnn_acc\n",
        "#print(f\"\\nMetalearner improvement: +{improvement:.2f}%\")\n",
        "\n",
        "# Save RQ2 table\n",
        "rq2_df = pd.DataFrame({\n",
        "    'Model': list(rq2_results.keys()),\n",
        "    'Validation Accuracy (%)': [f\"{v:.2f}\" for v in rq2_results.values()]\n",
        "})\n",
        "rq2_df.to_excel(f'{CONFIG[\"tables_dir\"]}/RQ2/RQ2_Tab1.xlsx', index=False)\n",
        "print(\"\\nRQ2 table saved\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xRNgHazaRzqM",
        "outputId": "48c70da9-29ca-4853-92f6-74c08b6d3282"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RQ2: Metalearner vs CNN Comparison\n",
            "==================================================\n",
            "CNN Only (EfficientNet-B3)    : 90.45%\n",
            "Random Forest                 : 91.57%\n",
            "XGBoost                       : 89.89%\n",
            "LightGBM                      : 90.45%\n",
            "SVM                           : 91.01%\n",
            "Metalearner (Stacking)        : 89.89%\n",
            "\n",
            "RQ2 table saved\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# C15b: RQ2 - Base Learner Comparison Chart\n",
        "print(\"Creating RQ2 base learner comparison chart...\")\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "\n",
        "base_learners = ['Random Forest', 'XGBoost', 'LightGBM', 'SVM', 'Meta-LR\\n(Stacking)']\n",
        "base_accuracies = [rf_val_acc * 100, xgb_val_acc * 100, lgb_val_acc * 100,\n",
        "                   svm_val_acc * 100, meta_val_acc * 100]\n",
        "\n",
        "colors = ['#E0E0E0', '#C0C0C0', '#A0A0A0', '#808080', '#505050']\n",
        "patterns = ['///', '...', '\\\\\\\\\\\\', '|||', 'xxx']\n",
        "\n",
        "bars = ax.bar(base_learners, base_accuracies, color=colors, edgecolor='black', linewidth=1.5)\n",
        "for bar, pattern in zip(bars, patterns):\n",
        "    bar.set_hatch(pattern)\n",
        "\n",
        "for bar, acc in zip(bars, base_accuracies):\n",
        "    ax.text(bar.get_x() + bar.get_width()/2., bar.get_height() + 0.3,\n",
        "            f'{acc:.2f}%', ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
        "\n",
        "ax.set_ylabel('Validation Accuracy (%)', fontsize=12)\n",
        "ax.set_title('Base Learner Performance Comparison (RQ2)', fontsize=14, fontweight='bold')\n",
        "ax.set_ylim(85, 95)\n",
        "ax.grid(axis='y', alpha=0.3)\n",
        "\n",
        "# Add horizontal line for CNN baseline\n",
        "ax.axhline(y=best_cnn_acc, color='red', linestyle='--', linewidth=2, label=f'CNN Baseline ({best_cnn_acc:.2f}%)')\n",
        "ax.legend(loc='lower right')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(f'{CONFIG[\"figures_dir\"]}/RQ2/RQ2_Fig3.pdf', dpi=300, bbox_inches='tight')\n",
        "plt.close()\n",
        "print(\"  Saved: RQ2_Fig3.pdf (base learner comparison)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_tgIOTYkq0K-",
        "outputId": "66e7b823-fab4-4293-c37a-57815644b7b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating RQ2 base learner comparison chart...\n",
            "  Saved: RQ2_Fig3.pdf (base learner comparison)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. Knowledge-Based Rule Engine Design"
      ],
      "metadata": {
        "id": "WNEQ-pHXXMcK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "9.1 Rule-based reasoning engine (RQ3)"
      ],
      "metadata": {
        "id": "ckF54SsLXNF0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# C16: Rule-based reasoning engine (RQ3)\n",
        "class SoilRuleEngine:\n",
        "    \"\"\"Agricultural domain knowledge for soil classification\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        # Soil type characteristics (simplified domain knowledge)\n",
        "        self.soil_rules = {\n",
        "            'Alluvial_Soil': {'texture': 'fine', 'color': 'light', 'fertility': 'high'},\n",
        "            'Arid_Soil': {'texture': 'coarse', 'color': 'light_brown', 'fertility': 'low'},\n",
        "            'Black_Soil': {'texture': 'fine', 'color': 'dark', 'fertility': 'high'},\n",
        "            'Laterite_Soil': {'texture': 'medium', 'color': 'red_brown', 'fertility': 'medium'},\n",
        "            'Mountain_Soil': {'texture': 'coarse', 'color': 'brown_gray', 'fertility': 'medium'},\n",
        "            'Red_Soil': {'texture': 'medium', 'color': 'red', 'fertility': 'medium'},\n",
        "            'Yellow_Soil': {'texture': 'medium', 'color': 'yellow', 'fertility': 'low'}\n",
        "        }\n",
        "\n",
        "    def apply_rules(self, cnn_prediction, meta_prediction, confidence):\n",
        "        \"\"\"Apply rule-based reasoning with confidence thresholds\"\"\"\n",
        "\n",
        "        # High confidence: trust the prediction\n",
        "        if confidence > 0.9:\n",
        "            return meta_prediction, confidence, \"High confidence\"\n",
        "\n",
        "        # Medium confidence: apply validation rules\n",
        "        elif confidence > 0.6:\n",
        "            # If CNN and metalearner agree\n",
        "            if cnn_prediction == meta_prediction:\n",
        "                return meta_prediction, confidence * 1.1, \"Agreement validation\"\n",
        "            else:\n",
        "                # Use metalearner (it's more accurate)\n",
        "                return meta_prediction, confidence, \"Metalearner preferred\"\n",
        "\n",
        "        # Low confidence: flag for review\n",
        "        else:\n",
        "            return meta_prediction, confidence, \"Manual review recommended\"\n",
        "\n",
        "    def get_crop_recommendation(self, soil_type):\n",
        "        \"\"\"Provide crop recommendations based on soil type\"\"\"\n",
        "        crops = {\n",
        "            'Alluvial_Soil': ['rice', 'wheat', 'sugarcane'],\n",
        "            'Arid_Soil': ['millet', 'sorghum', 'drought-resistant crops'],\n",
        "            'Black_Soil': ['cotton', 'wheat', 'sugarcane'],\n",
        "            'Laterite_Soil': ['tea', 'coffee', 'cashew'],\n",
        "            'Mountain_Soil': ['potato', 'barley', 'apple'],\n",
        "            'Red_Soil': ['groundnut', 'potato', 'fruits'],\n",
        "            'Yellow_Soil': ['rice', 'ragi', 'groundnut']\n",
        "        }\n",
        "        return crops.get(soil_type, ['consult agronomist'])\n",
        "\n",
        "# Initialize rule engine\n",
        "rule_engine = SoilRuleEngine()\n",
        "\n",
        "# Apply hybrid system using saved predictions\n",
        "print(\"Applying hybrid expert system (RQ3)\")\n",
        "\n",
        "# Use direct predictions from validation set\n",
        "cnn_val_preds = val_preds[:10]  # CNN predictions already extracted\n",
        "meta_preds = meta_learner.predict(val_meta_features[:10])\n",
        "meta_probs = meta_learner.predict_proba(val_meta_features[:10])\n",
        "\n",
        "print(\"\\nSample hybrid predictions:\")\n",
        "#print(\"=\" * 80)\n",
        "for i in range(5):\n",
        "    cnn_pred = cnn_val_preds[i]\n",
        "    meta_pred = meta_preds[i]\n",
        "    confidence = meta_probs[i].max()\n",
        "\n",
        "    final_pred, final_conf, reason = rule_engine.apply_rules(cnn_pred, meta_pred, confidence)\n",
        "\n",
        "    print(f\"\\nSample {i+1}:\")\n",
        "    print(f\"  CNN: {SOIL_CLASSES[cnn_pred]}\")\n",
        "    print(f\"  Metalearner: {SOIL_CLASSES[meta_pred]}\")\n",
        "    print(f\"  Final: {SOIL_CLASSES[final_pred]} (confidence: {final_conf:.2f})\")\n",
        "    print(f\"  Reason: {reason}\")\n",
        "    print(f\"  Recommended crops: {rule_engine.get_crop_recommendation(SOIL_CLASSES[final_pred])}\")\n",
        "\n",
        "print(\"\\nRule engine ready\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nViJA6-mWBaz",
        "outputId": "7fcb5561-e455-4d8c-b223-fce4da229e42"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Applying hybrid expert system (RQ3)\n",
            "\n",
            "Sample hybrid predictions:\n",
            "\n",
            "Sample 1:\n",
            "  CNN: Yellow_Soil\n",
            "  Metalearner: Yellow_Soil\n",
            "  Final: Yellow_Soil (confidence: 0.95)\n",
            "  Reason: High confidence\n",
            "  Recommended crops: ['rice', 'ragi', 'groundnut']\n",
            "\n",
            "Sample 2:\n",
            "  CNN: Black_Soil\n",
            "  Metalearner: Black_Soil\n",
            "  Final: Black_Soil (confidence: 0.99)\n",
            "  Reason: High confidence\n",
            "  Recommended crops: ['cotton', 'wheat', 'sugarcane']\n",
            "\n",
            "Sample 3:\n",
            "  CNN: Arid_Soil\n",
            "  Metalearner: Arid_Soil\n",
            "  Final: Arid_Soil (confidence: 0.99)\n",
            "  Reason: High confidence\n",
            "  Recommended crops: ['millet', 'sorghum', 'drought-resistant crops']\n",
            "\n",
            "Sample 4:\n",
            "  CNN: Arid_Soil\n",
            "  Metalearner: Arid_Soil\n",
            "  Final: Arid_Soil (confidence: 0.99)\n",
            "  Reason: High confidence\n",
            "  Recommended crops: ['millet', 'sorghum', 'drought-resistant crops']\n",
            "\n",
            "Sample 5:\n",
            "  CNN: Laterite_Soil\n",
            "  Metalearner: Laterite_Soil\n",
            "  Final: Laterite_Soil (confidence: 0.99)\n",
            "  Reason: High confidence\n",
            "  Recommended crops: ['tea', 'coffee', 'cashew']\n",
            "\n",
            "Rule engine ready\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. Hybrid Expert System Integration"
      ],
      "metadata": {
        "id": "ZEHCwXHTYCmW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "10.1 Evaluate hybrid system on full validation set (RQ3)"
      ],
      "metadata": {
        "id": "fKnKEn1CYDyp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# C17: Evaluate hybrid system on full validation set (RQ3)\n",
        "print(\"Evaluating hybrid system on full validation set\")\n",
        "\n",
        "# Get all predictions\n",
        "cnn_val_all_preds = val_preds\n",
        "meta_val_all_preds = meta_learner.predict(val_meta_features)\n",
        "meta_val_all_probs = meta_learner.predict_proba(val_meta_features)\n",
        "\n",
        "# Apply rules to all validation samples\n",
        "hybrid_preds = []\n",
        "hybrid_confs = []\n",
        "rule_reasons = []\n",
        "\n",
        "for i in range(len(val_labels)):\n",
        "    cnn_pred = cnn_val_all_preds[i]\n",
        "    meta_pred = meta_val_all_preds[i]\n",
        "    confidence = meta_val_all_probs[i].max()\n",
        "\n",
        "    final_pred, final_conf, reason = rule_engine.apply_rules(cnn_pred, meta_pred, confidence)\n",
        "\n",
        "    hybrid_preds.append(final_pred)\n",
        "    hybrid_confs.append(final_conf)\n",
        "    rule_reasons.append(reason)\n",
        "\n",
        "hybrid_preds = np.array(hybrid_preds)\n",
        "\n",
        "# Calculate accuracies\n",
        "cnn_only_acc = accuracy_score(val_labels, cnn_val_all_preds) * 100\n",
        "meta_only_acc = accuracy_score(val_labels, meta_val_all_preds) * 100\n",
        "hybrid_acc = accuracy_score(val_labels, hybrid_preds) * 100\n",
        "\n",
        "# RQ3 results\n",
        "rq3_results = {\n",
        "    'CNN Only': cnn_only_acc,\n",
        "    'Metalearner Only': meta_only_acc,\n",
        "    'Hybrid (CNN + Meta + Rules)': hybrid_acc\n",
        "}\n",
        "\n",
        "print(\"\\nRQ3: Rule-based integration results\")\n",
        "#print(\"=\" * 50)\n",
        "for model_name, val_acc in rq3_results.items():\n",
        "    print(f\"{model_name:30s}: {val_acc:.2f}%\")\n",
        "\n",
        "print(f\"\\nRule-based improvement: +{hybrid_acc - meta_only_acc:.2f}%\")\n",
        "\n",
        "# Save RQ3 table\n",
        "rq3_df = pd.DataFrame({\n",
        "    'Configuration': list(rq3_results.keys()),\n",
        "    'Validation Accuracy (%)': [f\"{v:.2f}\" for v in rq3_results.values()]\n",
        "})\n",
        "rq3_df.to_excel(f'{CONFIG[\"tables_dir\"]}/RQ3/RQ3_Tab1.xlsx', index=False)\n",
        "print(\"\\nRQ3 table saved\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zp8sbNsWXP8p",
        "outputId": "3bd8a3a1-110f-4100-bff5-83e02d0fa0ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating hybrid system on full validation set\n",
            "\n",
            "RQ3: Rule-based integration results\n",
            "CNN Only                      : 89.89%\n",
            "Metalearner Only              : 89.89%\n",
            "Hybrid (CNN + Meta + Rules)   : 89.89%\n",
            "\n",
            "Rule-based improvement: +0.00%\n",
            "\n",
            "RQ3 table saved\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# C17b: RQ3 - Rule Application Distribution Chart\n",
        "print(\"Creating RQ3 rule application distribution chart...\")\n",
        "\n",
        "from collections import Counter\n",
        "\n",
        "# Count rule reasons\n",
        "reason_counts = Counter(rule_reasons)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "\n",
        "reasons = list(reason_counts.keys())\n",
        "counts = list(reason_counts.values())\n",
        "\n",
        "colors = ['#D0D0D0', '#A0A0A0', '#707070']\n",
        "patterns = ['///', '...', '\\\\\\\\\\\\']\n",
        "\n",
        "# Ensure we have enough colors/patterns\n",
        "while len(colors) < len(reasons):\n",
        "    colors.append('#505050')\n",
        "    patterns.append('|||')\n",
        "\n",
        "bars = ax.bar(reasons, counts, color=colors[:len(reasons)], edgecolor='black', linewidth=1.5)\n",
        "for bar, pattern in zip(bars, patterns[:len(reasons)]):\n",
        "    bar.set_hatch(pattern)\n",
        "\n",
        "for bar, count in zip(bars, counts):\n",
        "    ax.text(bar.get_x() + bar.get_width()/2., bar.get_height() + 1,\n",
        "            f'{count}', ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
        "\n",
        "ax.set_ylabel('Number of Samples', fontsize=12)\n",
        "ax.set_xlabel('Rule Applied', fontsize=12)\n",
        "ax.set_title('Distribution of Rule Applications in Hybrid System (RQ3)', fontsize=14, fontweight='bold')\n",
        "ax.grid(axis='y', alpha=0.3)\n",
        "\n",
        "# Add percentage labels\n",
        "total = sum(counts)\n",
        "for bar, count in zip(bars, counts):\n",
        "    pct = count / total * 100\n",
        "    ax.text(bar.get_x() + bar.get_width()/2., bar.get_height()/2,\n",
        "            f'{pct:.1f}%', ha='center', va='center', fontsize=10, color='black')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(f'{CONFIG[\"figures_dir\"]}/RQ3/RQ3_Fig3.pdf', dpi=300, bbox_inches='tight')\n",
        "plt.close()\n",
        "print(\"  Saved: RQ3_Fig3.pdf (rule application distribution)\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qSIdTNxvq9jY",
        "outputId": "05e09d13-e997-40c9-dcfa-0633f0677a85"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating RQ3 rule application distribution chart...\n",
            "  Saved: RQ3_Fig3.pdf (rule application distribution)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "10.2 Explainability - GradCAM implementation (RQ4)"
      ],
      "metadata": {
        "id": "JeDDxGdR20sw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# C18: Explainability - GradCAM implementation (RQ4)\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class GradCAM:\n",
        "    \"\"\"GradCAM for CNN explainability\"\"\"\n",
        "\n",
        "    def __init__(self, model):\n",
        "        self.model = model\n",
        "        self.gradients = None\n",
        "        self.activations = None\n",
        "\n",
        "    def save_gradient(self, grad):\n",
        "        self.gradients = grad\n",
        "\n",
        "    def forward_hook(self, module, input, output):\n",
        "        self.activations = output\n",
        "\n",
        "    def backward_hook(self, module, grad_input, grad_output):\n",
        "        self.gradients = grad_output[0]\n",
        "\n",
        "    def generate_cam(self, input_image, target_class):\n",
        "        \"\"\"Generate GradCAM heatmap\"\"\"\n",
        "\n",
        "        # Get target layer (last conv layer for EfficientNet)\n",
        "        if hasattr(self.model, 'features'):  # EfficientNet\n",
        "            target_layer = self.model.features[-1]\n",
        "        elif hasattr(self.model, 'layer4'):  # ResNet\n",
        "            target_layer = self.model.layer4[-1]\n",
        "        else:\n",
        "            return None\n",
        "\n",
        "        # Register hooks\n",
        "        forward_handle = target_layer.register_forward_hook(self.forward_hook)\n",
        "        backward_handle = target_layer.register_full_backward_hook(self.backward_hook)\n",
        "\n",
        "        # Forward pass\n",
        "        self.model.eval()\n",
        "        output = self.model(input_image)\n",
        "\n",
        "        # Backward pass\n",
        "        self.model.zero_grad()\n",
        "        target = output[0, target_class]\n",
        "        target.backward()\n",
        "\n",
        "        # Generate CAM\n",
        "        gradients = self.gradients.detach().cpu()\n",
        "        activations = self.activations.detach().cpu()\n",
        "\n",
        "        weights = torch.mean(gradients, dim=(2, 3), keepdim=True)\n",
        "        cam = torch.sum(weights * activations, dim=1, keepdim=True)\n",
        "        cam = F.relu(cam)\n",
        "        cam = F.interpolate(cam, size=(224, 224), mode='bilinear', align_corners=False)\n",
        "        cam = cam.squeeze().numpy()\n",
        "\n",
        "        # Normalize\n",
        "        cam = (cam - cam.min()) / (cam.max() - cam.min() + 1e-8)\n",
        "\n",
        "        # Remove hooks\n",
        "        forward_handle.remove()\n",
        "        backward_handle.remove()\n",
        "\n",
        "        return cam\n",
        "\n",
        "# Initialize GradCAM\n",
        "gradcam = GradCAM(best_model)\n",
        "\n",
        "print(\"GradCAM initialized for explainability (RQ4)\")\n",
        "print(\"Ready to generate heatmaps for sample predictions\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fJdRoKO-YF8f",
        "outputId": "6eac8c1c-f993-470d-fd62-344380e1fc0b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GradCAM initialized for explainability (RQ4)\n",
            "Ready to generate heatmaps for sample predictions\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "10.3 Generate and save GradCAM visualizations (RQ4)"
      ],
      "metadata": {
        "id": "Fq1D7Q0r2760"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# C19: Generate and save GradCAM visualizations (RQ4)\n",
        "def visualize_gradcam(image_path, true_label, pred_label, cam, save_path):\n",
        "    \"\"\"Create GradCAM visualization\"\"\"\n",
        "\n",
        "    # Load original image\n",
        "    img = cv2.imread(image_path)\n",
        "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "    img = cv2.resize(img, (224, 224))\n",
        "\n",
        "    # Create heatmap\n",
        "    heatmap = cv2.applyColorMap(np.uint8(255 * cam), cv2.COLORMAP_JET)\n",
        "    heatmap = cv2.cvtColor(heatmap, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    # Overlay\n",
        "    overlay = cv2.addWeighted(img, 0.6, heatmap, 0.4, 0)\n",
        "\n",
        "    # Plot\n",
        "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
        "\n",
        "    axes[0].imshow(img)\n",
        "    axes[0].set_title('Original Image', fontsize=12)\n",
        "    axes[0].axis('off')\n",
        "\n",
        "    axes[1].imshow(cam, cmap='jet')\n",
        "    axes[1].set_title('GradCAM Heatmap', fontsize=12)\n",
        "    axes[1].axis('off')\n",
        "\n",
        "    axes[2].imshow(overlay)\n",
        "    axes[2].set_title(f'Overlay\\nTrue: {SOIL_CLASSES[true_label]}\\nPred: {SOIL_CLASSES[pred_label]}',\n",
        "                     fontsize=12)\n",
        "    axes[2].axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
        "    plt.close()\n",
        "\n",
        "# Generate GradCAM for sample images\n",
        "print(\"Generating GradCAM visualizations\")\n",
        "\n",
        "best_model.to(device)\n",
        "best_model.eval()\n",
        "\n",
        "num_samples = 3\n",
        "sample_indices = np.random.choice(len(val_dataset), num_samples, replace=False)\n",
        "\n",
        "for idx, sample_idx in enumerate(sample_indices):\n",
        "    # Get image and prediction\n",
        "    image, true_label = val_dataset[sample_idx]\n",
        "    image_path = val_dataset.image_paths[sample_idx]\n",
        "\n",
        "    # Get prediction\n",
        "    with torch.no_grad():\n",
        "        output = best_model(image.unsqueeze(0).to(device))\n",
        "        pred_label = output.argmax(1).item()\n",
        "\n",
        "    # Generate GradCAM\n",
        "    cam = gradcam.generate_cam(image.unsqueeze(0).to(device), pred_label)\n",
        "\n",
        "    # Save visualization\n",
        "    save_path = f'{CONFIG[\"figures_dir\"]}/RQ4/RQ4_Fig{idx+1}.pdf'\n",
        "    visualize_gradcam(image_path, true_label, pred_label, cam, save_path)\n",
        "\n",
        "    print(f\"  Saved: RQ4_Fig{idx+1}.pdf (True: {SOIL_CLASSES[true_label]}, Pred: {SOIL_CLASSES[pred_label]})\")\n",
        "\n",
        "print(\"\\nGradCAM visualizations saved\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E9c24dYwYOvF",
        "outputId": "a59d715c-3441-4653-d5c6-b1523e6d2100"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating GradCAM visualizations\n",
            "  Saved: RQ4_Fig1.pdf (True: Black_Soil, Pred: Black_Soil)\n",
            "  Saved: RQ4_Fig2.pdf (True: Laterite_Soil, Pred: Laterite_Soil)\n",
            "  Saved: RQ4_Fig3.pdf (True: Laterite_Soil, Pred: Laterite_Soil)\n",
            "\n",
            "GradCAM visualizations saved\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "10.4 SHAP analysis for metalearner explainability (RQ4)"
      ],
      "metadata": {
        "id": "LyY0T5cf3E0E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# C20: SHAP analysis for metalearner explainability (RQ4)\n",
        "import shap\n",
        "\n",
        "print(\"Computing SHAP values for metalearner\")\n",
        "\n",
        "# Use a subset for SHAP (computationally expensive)\n",
        "sample_size = 50\n",
        "shap_indices = np.random.choice(len(train_meta_features), sample_size, replace=False)\n",
        "X_shap = train_meta_features[shap_indices]\n",
        "\n",
        "# Create SHAP explainer\n",
        "explainer = shap.Explainer(meta_learner, X_shap)\n",
        "shap_values = explainer(val_meta_features[:50])\n",
        "\n",
        "# Summary plot\n",
        "plt.figure(figsize=(10, 6))\n",
        "shap.summary_plot(shap_values, val_meta_features[:50], show=False,\n",
        "                  feature_names=[f'Feature_{i}' for i in range(val_meta_features.shape[1])])\n",
        "plt.title('SHAP Feature Importance for Metalearner', fontsize=14)\n",
        "plt.tight_layout()\n",
        "plt.savefig(f'{CONFIG[\"figures_dir\"]}/RQ4/RQ4_Fig4.pdf', dpi=150, bbox_inches='tight')\n",
        "plt.close()\n",
        "\n",
        "print(\"  Saved: RQ4_Fig4.pdf (SHAP summary plot)\")\n",
        "\n",
        "# Bar plot - simplified\n",
        "mean_shap = np.abs(shap_values.values).mean(axis=0).mean(axis=1)  # Average across classes and samples\n",
        "top_10_idx = np.argsort(mean_shap)[-10:][::-1]\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.barh(range(10), mean_shap[top_10_idx])\n",
        "plt.yticks(range(10), [f'Feature_{i}' for i in top_10_idx])\n",
        "plt.xlabel('Mean |SHAP value|', fontsize=12)\n",
        "plt.title('Top 10 Important Features (SHAP)', fontsize=14)\n",
        "plt.tight_layout()\n",
        "plt.savefig(f'{CONFIG[\"figures_dir\"]}/RQ4/RQ4_Fig5.pdf', dpi=150, bbox_inches='tight')\n",
        "plt.close()\n",
        "\n",
        "print(\"  Saved: RQ4_Fig5.pdf (SHAP bar plot)\")\n",
        "\n",
        "# Create RQ4 summary table\n",
        "rq4_data = {\n",
        "    'Explainability Method': ['GradCAM (CNN)', 'SHAP (Metalearner)'],\n",
        "    'Purpose': ['Visual attention maps', 'Feature importance'],\n",
        "    'Samples Generated': [3, 50]\n",
        "}\n",
        "rq4_df = pd.DataFrame(rq4_data)\n",
        "rq4_df.to_excel(f'{CONFIG[\"tables_dir\"]}/RQ4/RQ4_Tab1.xlsx', index=False)\n",
        "\n",
        "print(\"\\nRQ4 explainability analysis complete\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yLr0o7uEYipn",
        "outputId": "4cb23653-d425-473e-a82e-0fc479664d9a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Computing SHAP values for metalearner\n",
            "  Saved: RQ4_Fig4.pdf (SHAP summary plot)\n",
            "  Saved: RQ4_Fig5.pdf (SHAP bar plot)\n",
            "\n",
            "RQ4 explainability analysis complete\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "10.5 Deployment feasibility analysis (RQ5)"
      ],
      "metadata": {
        "id": "DmoHAOGI3OUY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# C21: Deployment feasibility analysis (RQ5)\n",
        "import time\n",
        "\n",
        "print(\"Evaluating deployment feasibility (RQ5)\")\n",
        "\n",
        "# 1. Inference speed measurement\n",
        "def measure_inference_time(model, dataloader, num_batches=10):\n",
        "    \"\"\"Measure average inference time\"\"\"\n",
        "    model.eval()\n",
        "    model = model.to(device)\n",
        "\n",
        "    times = []\n",
        "    with torch.no_grad():\n",
        "        for i, (images, _) in enumerate(dataloader):\n",
        "            if i >= num_batches:\n",
        "                break\n",
        "\n",
        "            images = images.to(device)\n",
        "            start = time.time()\n",
        "            _ = model(images)\n",
        "            end = time.time()\n",
        "            times.append(end - start)\n",
        "\n",
        "    return np.mean(times) * 1000  # Convert to ms\n",
        "\n",
        "# Measure inference times\n",
        "print(\"\\nMeasuring inference times\")\n",
        "resnet_time = measure_inference_time(resnet_model, test_loader)\n",
        "efficientnet_time = measure_inference_time(efficientnet_model, test_loader)\n",
        "vit_time = measure_inference_time(vit_model, test_loader)\n",
        "\n",
        "print(f\"  ResNet50: {resnet_time:.2f} ms/batch\")\n",
        "print(f\"  EfficientNet-B3: {efficientnet_time:.2f} ms/batch\")\n",
        "print(f\"  Vision Transformer: {vit_time:.2f} ms/batch\")\n",
        "\n",
        "# 2. Model size\n",
        "def get_model_size(model):\n",
        "    \"\"\"Get model size in MB\"\"\"\n",
        "    param_size = sum(p.numel() * p.element_size() for p in model.parameters())\n",
        "    buffer_size = sum(b.numel() * b.element_size() for b in model.buffers())\n",
        "    return (param_size + buffer_size) / (1024 ** 2)\n",
        "\n",
        "resnet_size = get_model_size(resnet_model)\n",
        "efficientnet_size = get_model_size(efficientnet_model)\n",
        "vit_size = get_model_size(vit_model)\n",
        "\n",
        "print(f\"\\nModel sizes:\")\n",
        "print(f\"  ResNet50: {resnet_size:.2f} MB\")\n",
        "print(f\"  EfficientNet-B3: {efficientnet_size:.2f} MB\")\n",
        "print(f\"  Vision Transformer: {vit_size:.2f} MB\")\n",
        "\n",
        "# 3. Confidence calibration - Expected Calibration Error (ECE)\n",
        "def compute_ece(predictions, confidences, labels, n_bins=10):\n",
        "    \"\"\"Compute Expected Calibration Error\"\"\"\n",
        "    bin_boundaries = np.linspace(0, 1, n_bins + 1)\n",
        "    bin_lowers = bin_boundaries[:-1]\n",
        "    bin_uppers = bin_boundaries[1:]\n",
        "\n",
        "    ece = 0\n",
        "    for bin_lower, bin_upper in zip(bin_lowers, bin_uppers):\n",
        "        in_bin = (confidences > bin_lower) & (confidences <= bin_upper)\n",
        "        prop_in_bin = in_bin.mean()\n",
        "\n",
        "        if prop_in_bin > 0:\n",
        "            accuracy_in_bin = (predictions[in_bin] == labels[in_bin]).mean()\n",
        "            avg_confidence_in_bin = confidences[in_bin].mean()\n",
        "            ece += np.abs(avg_confidence_in_bin - accuracy_in_bin) * prop_in_bin\n",
        "\n",
        "    return ece\n",
        "\n",
        "# Compute ECE for hybrid system\n",
        "hybrid_confidences = np.array(hybrid_confs)\n",
        "ece = compute_ece(hybrid_preds, hybrid_confidences, val_labels)\n",
        "\n",
        "print(f\"\\nConfidence calibration:\")\n",
        "print(f\"  Expected Calibration Error (ECE): {ece:.4f}\")\n",
        "\n",
        "# 4. Create RQ5 summary table\n",
        "rq5_data = {\n",
        "    'Model': ['ResNet50', 'EfficientNet-B3', 'Vision Transformer', 'Hybrid System'],\n",
        "    'Inference Time (ms)': [f\"{resnet_time:.2f}\", f\"{efficientnet_time:.2f}\",\n",
        "                            f\"{vit_time:.2f}\", f\"{efficientnet_time:.2f}\"],\n",
        "    'Model Size (MB)': [f\"{resnet_size:.2f}\", f\"{efficientnet_size:.2f}\",\n",
        "                        f\"{vit_size:.2f}\", f\"{efficientnet_size:.2f}\"],\n",
        "    'Accuracy (%)': [f\"{resnet_val_acc:.2f}\", f\"{efficientnet_val_acc:.2f}\",\n",
        "                     f\"{vit_val_acc:.2f}\", f\"{hybrid_acc:.2f}\"]\n",
        "}\n",
        "\n",
        "rq5_df = pd.DataFrame(rq5_data)\n",
        "rq5_df.to_excel(f'{CONFIG[\"tables_dir\"]}/RQ5/RQ5_Tab1.xlsx', index=False)\n",
        "\n",
        "print(\"\\nRQ5 table saved!\")\n",
        "print(\"\\nDeployment analysis complete\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wrv218_cZUPJ",
        "outputId": "64d23afb-4264-4dbc-a22a-e7417379aa77"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating deployment feasibility (RQ5)\n",
            "\n",
            "Measuring inference times\n",
            "  ResNet50: 16.57 ms/batch\n",
            "  EfficientNet-B3: 42.65 ms/batch\n",
            "  Vision Transformer: 19.38 ms/batch\n",
            "\n",
            "Model sizes:\n",
            "  ResNet50: 89.93 MB\n",
            "  EfficientNet-B3: 41.18 MB\n",
            "  Vision Transformer: 327.32 MB\n",
            "\n",
            "Confidence calibration:\n",
            "  Expected Calibration Error (ECE): 0.0649\n",
            "\n",
            "RQ5 table saved!\n",
            "\n",
            "Deployment analysis complete\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# C21b: RQ5 - Inference Time Comparison Chart\n",
        "print(\"Creating RQ5 figures...\")\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "\n",
        "models_deploy = ['ResNet50', 'EfficientNet-B3', 'ViT', 'Hybrid']\n",
        "inference_times = [resnet_time, efficientnet_time, vit_time, efficientnet_time]\n",
        "\n",
        "colors = ['#D0D0D0', '#A0A0A0', '#707070', '#404040']\n",
        "patterns = ['///', '...', '\\\\\\\\\\\\', '|||']\n",
        "\n",
        "bars = ax.bar(models_deploy, inference_times, color=colors, edgecolor='black', linewidth=1.5)\n",
        "for bar, pattern in zip(bars, patterns):\n",
        "    bar.set_hatch(pattern)\n",
        "\n",
        "for bar, time in zip(bars, inference_times):\n",
        "    ax.text(bar.get_x() + bar.get_width()/2., bar.get_height() + 100,\n",
        "            f'{time:.1f} ms', ha='center', va='bottom', fontsize=11)\n",
        "\n",
        "ax.set_ylabel('Inference Time (ms/batch)', fontsize=12)\n",
        "ax.set_title('Model Inference Time Comparison (CPU)', fontsize=14, fontweight='bold')\n",
        "ax.grid(axis='y', alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(f'{CONFIG[\"figures_dir\"]}/RQ5/RQ5_Fig1.pdf', dpi=300, bbox_inches='tight')\n",
        "plt.close()\n",
        "print(\"  Saved: RQ5_Fig1.pdf (inference time comparison)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lOfmB0Hvqbl1",
        "outputId": "8b8e01e4-9ead-4115-b26b-7475ab2c72f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating RQ5 figures...\n",
            "  Saved: RQ5_Fig1.pdf (inference time comparison)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# C21c: RQ5 - Model Size Comparison Chart\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "\n",
        "model_sizes = [resnet_size, efficientnet_size, vit_size, efficientnet_size]\n",
        "\n",
        "bars = ax.bar(models_deploy, model_sizes, color=colors, edgecolor='black', linewidth=1.5)\n",
        "for bar, pattern in zip(bars, patterns):\n",
        "    bar.set_hatch(pattern)\n",
        "\n",
        "for bar, size in zip(bars, model_sizes):\n",
        "    ax.text(bar.get_x() + bar.get_width()/2., bar.get_height() + 5,\n",
        "            f'{size:.1f} MB', ha='center', va='bottom', fontsize=11)\n",
        "\n",
        "ax.set_ylabel('Model Size (MB)', fontsize=12)\n",
        "ax.set_title('Model Size Comparison', fontsize=14, fontweight='bold')\n",
        "ax.grid(axis='y', alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(f'{CONFIG[\"figures_dir\"]}/RQ5/RQ5_Fig2.pdf', dpi=300, bbox_inches='tight')\n",
        "plt.close()\n",
        "print(\"  Saved: RQ5_Fig2.pdf (model size comparison)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hsRqx5IAqehP",
        "outputId": "c626b849-218c-4ed3-b295-3d2a522eeeb4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Saved: RQ5_Fig2.pdf (model size comparison)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# C21d: RQ5 - Efficiency vs Accuracy Trade-off Chart\n",
        "fig, ax = plt.subplots(figsize=(10, 7))\n",
        "\n",
        "accuracies = [resnet_test_acc, efficientnet_test_acc, vit_test_acc, hybrid_test_acc]\n",
        "sizes = [resnet_size, efficientnet_size, vit_size, efficientnet_size]\n",
        "times = [resnet_time, efficientnet_time, vit_time, efficientnet_time]\n",
        "\n",
        "scatter = ax.scatter(sizes, accuracies, s=[t/10 for t in times],\n",
        "                     c=['gray', 'darkgray', 'dimgray', 'black'],\n",
        "                     alpha=0.7, edgecolors='black', linewidths=2)\n",
        "\n",
        "for i, model in enumerate(models_deploy):\n",
        "    ax.annotate(model, (sizes[i], accuracies[i]),\n",
        "                xytext=(10, 10), textcoords='offset points',\n",
        "                fontsize=11, fontweight='bold')\n",
        "\n",
        "ax.set_xlabel('Model Size (MB)', fontsize=12)\n",
        "ax.set_ylabel('Test Accuracy (%)', fontsize=12)\n",
        "ax.set_title('Efficiency vs Accuracy Trade-off\\n(Bubble size ∝ inference time)',\n",
        "             fontsize=14, fontweight='bold')\n",
        "ax.grid(alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(f'{CONFIG[\"figures_dir\"]}/RQ5/RQ5_Fig3.pdf', dpi=300, bbox_inches='tight')\n",
        "plt.close()\n",
        "print(\"  Saved: RQ5_Fig3.pdf (efficiency vs accuracy)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dc2TV8Syqg64",
        "outputId": "9ff0f35f-361f-4ab8-c86c-b970d97584d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Saved: RQ5_Fig3.pdf (efficiency vs accuracy)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#C21e: RQ5 - Calibration/Reliability Diagram\n",
        "print(\"Creating RQ5 calibration diagram\")\n",
        "\n",
        "from sklearn.calibration import calibration_curve\n",
        "\n",
        "# Get probabilities from best model (EfficientNet)\n",
        "efficientnet_model.eval()\n",
        "all_probs = []\n",
        "all_true = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        outputs = efficientnet_model(images.to(device))\n",
        "        probs = torch.softmax(outputs, dim=1)\n",
        "        all_probs.extend(probs.cpu().numpy())\n",
        "        all_true.extend(labels.numpy())\n",
        "\n",
        "all_probs = np.array(all_probs)\n",
        "all_true = np.array(all_true)\n",
        "\n",
        "# Get max probability (confidence) and whether prediction was correct\n",
        "confidences = np.max(all_probs, axis=1)\n",
        "predictions = np.argmax(all_probs, axis=1)\n",
        "accuracies = (predictions == all_true).astype(int)\n",
        "\n",
        "# Compute calibration curve\n",
        "prob_true, prob_pred = calibration_curve(accuracies, confidences, n_bins=10, strategy='uniform')\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Reliability diagram\n",
        "ax1.plot([0, 1], [0, 1], 'k--', label='Perfectly Calibrated', linewidth=2)\n",
        "ax1.plot(prob_pred, prob_true, 'o-', color='#505050', linewidth=2, markersize=8, label='EfficientNet-B3')\n",
        "ax1.fill_between(prob_pred, prob_true, prob_pred, alpha=0.2, color='gray')\n",
        "\n",
        "ax1.set_xlabel('Mean Predicted Confidence', fontsize=12)\n",
        "ax1.set_ylabel('Fraction of Positives (Accuracy)', fontsize=12)\n",
        "ax1.set_title('Reliability Diagram', fontsize=14, fontweight='bold')\n",
        "ax1.legend(loc='lower right')\n",
        "ax1.grid(alpha=0.3)\n",
        "ax1.set_xlim([0, 1])\n",
        "ax1.set_ylim([0, 1])\n",
        "\n",
        "# Confidence histogram\n",
        "ax2.hist(confidences, bins=10, range=(0, 1), color='#808080', edgecolor='black',\n",
        "         linewidth=1.5, alpha=0.7)\n",
        "ax2.set_xlabel('Confidence', fontsize=12)\n",
        "ax2.set_ylabel('Count', fontsize=12)\n",
        "ax2.set_title('Confidence Distribution', fontsize=14, fontweight='bold')\n",
        "ax2.grid(axis='y', alpha=0.3)\n",
        "\n",
        "# Add ECE annotation\n",
        "ax1.text(0.05, 0.95, f'ECE = {ece:.4f}', transform=ax1.transAxes, fontsize=12,\n",
        "         verticalalignment='top', bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(f'{CONFIG[\"figures_dir\"]}/RQ5/RQ5_Fig4.pdf', dpi=300, bbox_inches='tight')\n",
        "plt.close()\n",
        "print(\"  Saved: RQ5_Fig4.pdf (calibration diagram)\")\n",
        "\n",
        "\n",
        "#print(\"\\n\" + \"=\"*70)\n",
        "print(\"All additional figures generated\")\n",
        "#print(\"=\"*70)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2pHnOWYprE6a",
        "outputId": "8cdcadba-5569-41b6-b06b-b08c5dc4b61c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating RQ5 calibration diagram\n",
            "  Saved: RQ5_Fig4.pdf (calibration diagram)\n",
            "All additional figures generated\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "10.6 Final evaluation on test set"
      ],
      "metadata": {
        "id": "3HRzjKM-3Zqc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# C22: Final evaluation on test set\n",
        "print(\"Evaluating all models on test set\")\n",
        "\n",
        "# 1. CNN models on test set\n",
        "def evaluate_model(model, dataloader):\n",
        "    \"\"\"Evaluate model and return accuracy, predictions, probabilities\"\"\"\n",
        "    model.eval()\n",
        "    model = model.to(device)\n",
        "\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    all_probs = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in dataloader:\n",
        "            images = images.to(device)\n",
        "            outputs = model(images)\n",
        "            probs = torch.softmax(outputs, dim=1)\n",
        "            _, preds = outputs.max(1)\n",
        "\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.numpy())\n",
        "            all_probs.extend(probs.cpu().numpy())\n",
        "\n",
        "    all_preds = np.array(all_preds)\n",
        "    all_labels = np.array(all_labels)\n",
        "    all_probs = np.array(all_probs)\n",
        "\n",
        "    accuracy = accuracy_score(all_labels, all_preds) * 100\n",
        "    f1 = f1_score(all_labels, all_preds, average='macro') * 100\n",
        "\n",
        "    return accuracy, f1, all_preds, all_probs, all_labels\n",
        "\n",
        "# Evaluate CNN models\n",
        "print(\"\\nCNN models:\")\n",
        "resnet_test_acc, resnet_test_f1, _, _, test_labels = evaluate_model(resnet_model, test_loader)\n",
        "print(f\"  ResNet50: Acc={resnet_test_acc:.2f}%, F1={resnet_test_f1:.2f}%\")\n",
        "\n",
        "efficientnet_test_acc, efficientnet_test_f1, cnn_test_preds, _, _ = evaluate_model(efficientnet_model, test_loader)\n",
        "print(f\"  EfficientNet-B3: Acc={efficientnet_test_acc:.2f}%, F1={efficientnet_test_f1:.2f}%\")\n",
        "\n",
        "vit_test_acc, vit_test_f1, _, _, _ = evaluate_model(vit_model, test_loader)\n",
        "print(f\"  Vision Transformer: Acc={vit_test_acc:.2f}%, F1={vit_test_f1:.2f}%\")\n",
        "\n",
        "# 2. Metalearner on test set\n",
        "print(\"\\nMetalearner:\")\n",
        "test_meta_features = np.column_stack([\n",
        "    rf.predict_proba(test_features),\n",
        "    xgb_model.predict_proba(test_features),\n",
        "    lgb_model.predict_proba(test_features),\n",
        "    svm.predict_proba(test_features)\n",
        "])\n",
        "\n",
        "meta_test_preds = meta_learner.predict(test_meta_features)\n",
        "meta_test_probs = meta_learner.predict_proba(test_meta_features)\n",
        "meta_test_acc = accuracy_score(test_labels, meta_test_preds) * 100\n",
        "meta_test_f1 = f1_score(test_labels, meta_test_preds, average='macro') * 100\n",
        "print(f\"  Metalearner: Acc={meta_test_acc:.2f}%, F1={meta_test_f1:.2f}%\")\n",
        "\n",
        "# 3. Hybrid system on test set\n",
        "print(\"\\nHybrid system:\")\n",
        "hybrid_test_preds = []\n",
        "for i in range(len(test_labels)):\n",
        "    cnn_pred = cnn_test_preds[i]\n",
        "    meta_pred = meta_test_preds[i]\n",
        "    confidence = meta_test_probs[i].max()\n",
        "\n",
        "    final_pred, _, _ = rule_engine.apply_rules(cnn_pred, meta_pred, confidence)\n",
        "    hybrid_test_preds.append(final_pred)\n",
        "\n",
        "hybrid_test_preds = np.array(hybrid_test_preds)\n",
        "hybrid_test_acc = accuracy_score(test_labels, hybrid_test_preds) * 100\n",
        "hybrid_test_f1 = f1_score(test_labels, hybrid_test_preds, average='macro') * 100\n",
        "print(f\"  Hybrid: Acc={hybrid_test_acc:.2f}%, F1={hybrid_test_f1:.2f}%\")\n",
        "\n",
        "# Save final results table\n",
        "final_results = {\n",
        "    'Model': ['ResNet50', 'EfficientNet-B3', 'ViT', 'Metalearner', 'Hybrid System'],\n",
        "    'Test Accuracy (%)': [f\"{resnet_test_acc:.2f}\", f\"{efficientnet_test_acc:.2f}\",\n",
        "                          f\"{vit_test_acc:.2f}\", f\"{meta_test_acc:.2f}\", f\"{hybrid_test_acc:.2f}\"],\n",
        "    'Test F1-Score (%)': [f\"{resnet_test_f1:.2f}\", f\"{efficientnet_test_f1:.2f}\",\n",
        "                          f\"{vit_test_f1:.2f}\", f\"{meta_test_f1:.2f}\", f\"{hybrid_test_f1:.2f}\"]\n",
        "}\n",
        "\n",
        "final_df = pd.DataFrame(final_results)\n",
        "final_df.to_excel(f'{CONFIG[\"tables_dir\"]}/RQ5/RQ5_Tab2.xlsx', index=False)\n",
        "\n",
        "print(\"\\nFinal results table saved\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kp5Sy2s7Zl-n",
        "outputId": "89b9021c-e799-4726-deff-0803ca7d2552"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating all models on test set\n",
            "\n",
            "CNN models:\n",
            "  ResNet50: Acc=80.34%, F1=71.06%\n",
            "  EfficientNet-B3: Acc=88.76%, F1=84.51%\n",
            "  Vision Transformer: Acc=74.16%, F1=65.65%\n",
            "\n",
            "Metalearner:\n",
            "  Metalearner: Acc=88.20%, F1=84.23%\n",
            "\n",
            "Hybrid system:\n",
            "  Hybrid: Acc=88.20%, F1=84.23%\n",
            "\n",
            "Final results table saved\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "10.7 Generate confusion matrices for all configurations"
      ],
      "metadata": {
        "id": "_m6ne_S53rfo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# C23: Generate confusion matrices for all configurations\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "print(\"Generating confusion matrices\")\n",
        "\n",
        "def plot_confusion_matrix(y_true, y_pred, title, save_path):\n",
        "    \"\"\"Plot and save confusion matrix\"\"\"\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                xticklabels=[c.replace('_Soil', '') for c in SOIL_CLASSES],\n",
        "                yticklabels=[c.replace('_Soil', '') for c in SOIL_CLASSES])\n",
        "    plt.title(title, fontsize=14)\n",
        "    plt.ylabel('True Label', fontsize=12)\n",
        "    plt.xlabel('Predicted Label', fontsize=12)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
        "    plt.close()\n",
        "\n",
        "# Get ResNet predictions on test set using dataloader\n",
        "resnet_model.eval()\n",
        "resnet_test_preds_list = []\n",
        "with torch.no_grad():\n",
        "    for images, _ in test_loader:\n",
        "        outputs = resnet_model(images.to(device))\n",
        "        preds = outputs.argmax(1).cpu().numpy()\n",
        "        resnet_test_preds_list.extend(preds)\n",
        "resnet_test_preds = np.array(resnet_test_preds_list)\n",
        "\n",
        "# Generate confusion matrices for RQ1, RQ2, RQ3\n",
        "# RQ1: CNN architectures\n",
        "plot_confusion_matrix(test_labels, resnet_test_preds,\n",
        "                     'Confusion Matrix - ResNet50',\n",
        "                     f'{CONFIG[\"figures_dir\"]}/RQ1/RQ1_Fig1.pdf')\n",
        "print(\"  Saved: RQ1_Fig1.pdf\")\n",
        "\n",
        "plot_confusion_matrix(test_labels, cnn_test_preds,\n",
        "                     'Confusion Matrix - EfficientNet-B3',\n",
        "                     f'{CONFIG[\"figures_dir\"]}/RQ1/RQ1_Fig2.pdf')\n",
        "print(\"  Saved: RQ1_Fig2.pdf\")\n",
        "\n",
        "# RQ2: Metalearner\n",
        "plot_confusion_matrix(test_labels, meta_test_preds,\n",
        "                     'Confusion Matrix - Metalearner',\n",
        "                     f'{CONFIG[\"figures_dir\"]}/RQ2/RQ2_Fig1.pdf')\n",
        "print(\"  Saved: RQ2_Fig1.pdf\")\n",
        "\n",
        "# RQ3: Hybrid system\n",
        "plot_confusion_matrix(test_labels, hybrid_test_preds,\n",
        "                     'Confusion Matrix - Hybrid System',\n",
        "                     f'{CONFIG[\"figures_dir\"]}/RQ3/RQ3_Fig1.pdf')\n",
        "print(\"  Saved: RQ3_Fig1.pdf\")\n",
        "\n",
        "print(\"\\nAll confusion matrices saved\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "abcJ9PJiay1Q",
        "outputId": "b210feb7-172a-49a4-d16b-c7001cddf54d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating confusion matrices\n",
            "  Saved: RQ1_Fig1.pdf\n",
            "  Saved: RQ1_Fig2.pdf\n",
            "  Saved: RQ2_Fig1.pdf\n",
            "  Saved: RQ3_Fig1.pdf\n",
            "\n",
            "All confusion matrices saved\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# C23b: ViT Confusion Matrix (Missing for RQ1)\n",
        "print(\"Creating ViT confusion matrix...\")\n",
        "\n",
        "vit_model.eval()\n",
        "vit_test_preds_list = []\n",
        "with torch.no_grad():\n",
        "    for images, _ in test_loader:\n",
        "        outputs = vit_model(images.to(device))\n",
        "        preds = outputs.argmax(1).cpu().numpy()\n",
        "        vit_test_preds_list.extend(preds)\n",
        "vit_test_preds_arr = np.array(vit_test_preds_list)\n",
        "\n",
        "plot_confusion_matrix(test_labels, vit_test_preds_arr,\n",
        "                     'Confusion Matrix - Vision Transformer (ViT)',\n",
        "                     f'{CONFIG[\"figures_dir\"]}/RQ1/RQ1_Fig4.pdf')\n",
        "print(\"  Saved: RQ1_Fig4.pdf (ViT confusion matrix)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hPEtMe-7pJid",
        "outputId": "d4c1ac5c-c750-4cf6-9fda-f856ab44093c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating ViT confusion matrix...\n",
            "  Saved: RQ1_Fig4.pdf (ViT confusion matrix)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "10.8 Comparative visualizations (accuracy comparison, training curves)"
      ],
      "metadata": {
        "id": "oQdcu-De3yCK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # C24: Comparative visualizations (accuracy comparison, training curves)\n",
        "# print(\"Creating comparative visualizations\")\n",
        "\n",
        "# # 1. Accuracy comparison bar chart (RQ1-RQ3)\n",
        "# fig, ax = plt.subplots(figsize=(12, 6))\n",
        "\n",
        "# models = ['ResNet50', 'EfficientNet-B3', 'ViT', 'Metalearner', 'Hybrid']\n",
        "# test_accuracies = [resnet_test_acc, efficientnet_test_acc, vit_test_acc,\n",
        "#                    meta_test_acc, hybrid_test_acc]\n",
        "\n",
        "# colors = ['#C5C5C5', '#AEAEAE', '#979797', '#808080', '#696969']\n",
        "# bars = ax.bar(models, test_accuracies, color=colors, alpha=0.8)\n",
        "\n",
        "# # Add value labels on bars\n",
        "# for bar in bars:\n",
        "#     height = bar.get_height()\n",
        "#     ax.text(bar.get_x() + bar.get_width()/2., height,\n",
        "#             f'{height:.2f}%', ha='center', va='bottom', fontsize=11)\n",
        "\n",
        "# ax.set_ylabel('Test Accuracy (%)', fontsize=12)\n",
        "# ax.set_title('Model Performance Comparison on Test Set', fontsize=14)\n",
        "# ax.set_ylim(0, 100)\n",
        "# ax.grid(axis='y', alpha=0.3)\n",
        "\n",
        "# plt.tight_layout()\n",
        "# plt.savefig(f'{CONFIG[\"figures_dir\"]}/RQ3/RQ3_Fig2.pdf', dpi=150, bbox_inches='tight')\n",
        "# plt.close()\n",
        "# print(\"  Saved: RQ3_Fig2.pdf (accuracy comparison)\")\n",
        "\n",
        "# # 2. Training curves for best CNN (EfficientNet)\n",
        "# fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# epochs_range = range(1, len(efficientnet_history['train_loss']) + 1)\n",
        "\n",
        "# # Loss curves\n",
        "# ax1.plot(epochs_range, efficientnet_history['train_loss'], 'b-', label='Train Loss', linewidth=2)\n",
        "# ax1.plot(epochs_range, efficientnet_history['val_loss'], 'r-', label='Val Loss', linewidth=2)\n",
        "# ax1.set_xlabel('Epoch', fontsize=12)\n",
        "# ax1.set_ylabel('Loss', fontsize=12)\n",
        "# ax1.set_title('Training and Validation Loss', fontsize=14)\n",
        "# ax1.legend(fontsize=11)\n",
        "# ax1.grid(alpha=0.3)\n",
        "\n",
        "# # Accuracy curves\n",
        "# ax2.plot(epochs_range, efficientnet_history['train_acc'], 'b-', label='Train Acc', linewidth=2)\n",
        "# ax2.plot(epochs_range, efficientnet_history['val_acc'], 'r-', label='Val Acc', linewidth=2)\n",
        "# ax2.set_xlabel('Epoch', fontsize=12)\n",
        "# ax2.set_ylabel('Accuracy (%)', fontsize=12)\n",
        "# ax2.set_title('Training and Validation Accuracy', fontsize=14)\n",
        "# ax2.legend(fontsize=11)\n",
        "# ax2.grid(alpha=0.3)\n",
        "\n",
        "# plt.tight_layout()\n",
        "# plt.savefig(f'{CONFIG[\"figures_dir\"]}/RQ1/RQ1_Fig3.pdf', dpi=150, bbox_inches='tight')\n",
        "# plt.close()\n",
        "# print(\"  Saved: RQ1_Fig3.pdf (training curves)\")\n",
        "\n",
        "# # 3. F1-Score comparison\n",
        "# fig, ax = plt.subplots(figsize=(12, 6))\n",
        "\n",
        "# f1_scores = [resnet_test_f1, efficientnet_test_f1, vit_test_f1,\n",
        "#              meta_test_f1, hybrid_test_f1]\n",
        "\n",
        "# bars = ax.bar(models, f1_scores, color=colors, alpha=0.8)\n",
        "\n",
        "# for bar in bars:\n",
        "#     height = bar.get_height()\n",
        "#     ax.text(bar.get_x() + bar.get_width()/2., height,\n",
        "#             f'{height:.2f}%', ha='center', va='bottom', fontsize=11)\n",
        "\n",
        "# ax.set_ylabel('F1-Score (%)', fontsize=12)\n",
        "# ax.set_title('F1-Score Comparison on Test Set', fontsize=14)\n",
        "# ax.set_ylim(0, 100)\n",
        "# ax.grid(axis='y', alpha=0.3)\n",
        "\n",
        "# plt.tight_layout()\n",
        "# plt.savefig(f'{CONFIG[\"figures_dir\"]}/RQ2/RQ2_Fig2.pdf', dpi=150, bbox_inches='tight')\n",
        "# plt.close()\n",
        "# print(\"  Saved: RQ2_Fig2.pdf (F1-score comparison)\")\n",
        "\n",
        "# print(\"\\nComparative visualizations complete\")"
      ],
      "metadata": {
        "id": "-kmwSXwUbNb2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 24: Comparative visualizations (accuracy comparison, training curves)\n",
        "print(\"Creating comparative visualizations with patterns\")\n",
        "\n",
        "# 1. Accuracy comparison bar chart (RQ1-RQ3) - WITH PATTERNS\n",
        "fig, ax = plt.subplots(figsize=(12, 6))\n",
        "\n",
        "models = ['ResNet50', 'EfficientNet-B3', 'ViT', 'Metalearner', 'Hybrid']\n",
        "test_accuracies = [resnet_test_acc, efficientnet_test_acc, vit_test_acc,\n",
        "                   meta_test_acc, hybrid_test_acc]\n",
        "\n",
        "# Use grayscale colors with distinct patterns (professor feedback: avoid excessive color)\n",
        "colors = ['#D8D8D8', '#C0C0C0', '#A8A8A8', '#B8B8B8', '#A0A0A0']  # Gray tones\n",
        "patterns = ['///', '...', '\\\\\\\\\\\\', '|||', 'xxx']  # Distinct patterns\n",
        "\n",
        "bars = ax.bar(models, test_accuracies, color=colors, edgecolor='black', linewidth=1.5, width=0.6)\n",
        "\n",
        "# Add patterns to bars\n",
        "for bar, pattern in zip(bars, patterns):\n",
        "    bar.set_hatch(pattern)\n",
        "\n",
        "# Add value labels on bars\n",
        "for bar, acc in zip(bars, test_accuracies):\n",
        "    height = bar.get_height()\n",
        "    # Highlight best with star\n",
        "    label = f'{height:.2f}%'\n",
        "    if height == max(test_accuracies):\n",
        "        label += ' ★'\n",
        "    ax.text(bar.get_x() + bar.get_width()/2., height + 1,\n",
        "            label, ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
        "\n",
        "ax.set_ylabel('Test Accuracy (%)', fontsize=13, fontweight='bold')\n",
        "ax.set_title('Model Performance Comparison on Test Set', fontsize=15, fontweight='bold', pad=15)\n",
        "ax.set_ylim(0, 100)\n",
        "ax.grid(axis='y', alpha=0.3, linestyle='--', color='gray')\n",
        "ax.set_axisbelow(True)\n",
        "\n",
        "# Remove top and right spines for cleaner look\n",
        "ax.spines['top'].set_visible(False)\n",
        "ax.spines['right'].set_visible(False)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(f'{CONFIG[\"figures_dir\"]}/RQ3/RQ3_Fig2.pdf', dpi=300, bbox_inches='tight')\n",
        "plt.close()\n",
        "print(\"  Saved: RQ3_Fig2.pdf (accuracy comparison with patterns)\")\n",
        "\n",
        "# 2. Training curves for best CNN (EfficientNet) - GRAYSCALE\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "epochs_range = range(1, len(efficientnet_history['train_loss']) + 1)\n",
        "\n",
        "# Loss curves - using different line styles instead of colors\n",
        "ax1.plot(epochs_range, efficientnet_history['train_loss'], 'k-',\n",
        "         label='Train Loss', linewidth=2, marker='o', markersize=4, markevery=2)\n",
        "ax1.plot(epochs_range, efficientnet_history['val_loss'], 'k--',\n",
        "         label='Val Loss', linewidth=2, marker='s', markersize=4, markevery=2)\n",
        "ax1.set_xlabel('Epoch', fontsize=12, fontweight='bold')\n",
        "ax1.set_ylabel('Loss', fontsize=12, fontweight='bold')\n",
        "ax1.set_title('Training and Validation Loss', fontsize=14, fontweight='bold')\n",
        "ax1.legend(fontsize=11)\n",
        "ax1.grid(alpha=0.3, linestyle='--', color='gray')\n",
        "ax1.spines['top'].set_visible(False)\n",
        "ax1.spines['right'].set_visible(False)\n",
        "\n",
        "# Accuracy curves - using different line styles instead of colors\n",
        "ax2.plot(epochs_range, efficientnet_history['train_acc'], 'k-',\n",
        "         label='Train Acc', linewidth=2, marker='o', markersize=4, markevery=2)\n",
        "ax2.plot(epochs_range, efficientnet_history['val_acc'], 'k--',\n",
        "         label='Val Acc', linewidth=2, marker='s', markersize=4, markevery=2)\n",
        "ax2.set_xlabel('Epoch', fontsize=12, fontweight='bold')\n",
        "ax2.set_ylabel('Accuracy (%)', fontsize=12, fontweight='bold')\n",
        "ax2.set_title('Training and Validation Accuracy', fontsize=14, fontweight='bold')\n",
        "ax2.legend(fontsize=11)\n",
        "ax2.grid(alpha=0.3, linestyle='--', color='gray')\n",
        "ax2.spines['top'].set_visible(False)\n",
        "ax2.spines['right'].set_visible(False)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(f'{CONFIG[\"figures_dir\"]}/RQ1/RQ1_Fig3.pdf', dpi=300, bbox_inches='tight')\n",
        "plt.close()\n",
        "print(\"  Saved: RQ1_Fig3.pdf (training curves - grayscale)\")\n",
        "\n",
        "# 3. F1-Score comparison - WITH PATTERNS\n",
        "fig, ax = plt.subplots(figsize=(12, 6))\n",
        "\n",
        "f1_scores = [resnet_test_f1, efficientnet_test_f1, vit_test_f1,\n",
        "             meta_test_f1, hybrid_test_f1]\n",
        "\n",
        "bars = ax.bar(models, f1_scores, color=colors, edgecolor='black', linewidth=1.5, width=0.6)\n",
        "\n",
        "# Add patterns to bars (same as accuracy chart)\n",
        "for bar, pattern in zip(bars, patterns):\n",
        "    bar.set_hatch(pattern)\n",
        "\n",
        "for bar, f1 in zip(bars, f1_scores):\n",
        "    height = bar.get_height()\n",
        "    label = f'{height:.2f}%'\n",
        "    if height == max(f1_scores):\n",
        "        label += ' ★'\n",
        "    ax.text(bar.get_x() + bar.get_width()/2., height + 1,\n",
        "            label, ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
        "\n",
        "ax.set_ylabel('F1-Score (%)', fontsize=13, fontweight='bold')\n",
        "ax.set_title('F1-Score Comparison on Test Set', fontsize=15, fontweight='bold', pad=15)\n",
        "ax.set_ylim(0, 100)\n",
        "ax.grid(axis='y', alpha=0.3, linestyle='--', color='gray')\n",
        "ax.set_axisbelow(True)\n",
        "\n",
        "# Remove top and right spines\n",
        "ax.spines['top'].set_visible(False)\n",
        "ax.spines['right'].set_visible(False)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(f'{CONFIG[\"figures_dir\"]}/RQ2/RQ2_Fig2.pdf', dpi=300, bbox_inches='tight')\n",
        "plt.close()\n",
        "print(\"  Saved: RQ2_Fig2.pdf (F1-score comparison with patterns)\")\n",
        "\n",
        "print(\"\\nComparative visualizations complete (pattern-based)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8wbJbnE2jcJH",
        "outputId": "feb3190b-d770-4389-85a0-98c9d9630c39"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating comparative visualizations with patterns\n",
            "  Saved: RQ3_Fig2.pdf (accuracy comparison with patterns)\n",
            "  Saved: RQ1_Fig3.pdf (training curves - grayscale)\n",
            "  Saved: RQ2_Fig2.pdf (F1-score comparison with patterns)\n",
            "\n",
            "Comparative visualizations complete (pattern-based)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# C24b: Per-Class F1 Comparison Chart\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "print(\"Creating per-class F1 comparison...\")\n",
        "\n",
        "resnet_report = classification_report(test_labels, resnet_test_preds, output_dict=True)\n",
        "efficientnet_report = classification_report(test_labels, cnn_test_preds, output_dict=True)\n",
        "vit_report = classification_report(test_labels, vit_test_preds_arr, output_dict=True)\n",
        "\n",
        "classes_short = [c.replace('_Soil', '') for c in SOIL_CLASSES]\n",
        "resnet_f1 = [resnet_report[str(i)]['f1-score'] * 100 for i in range(len(SOIL_CLASSES))]\n",
        "efficientnet_f1 = [efficientnet_report[str(i)]['f1-score'] * 100 for i in range(len(SOIL_CLASSES))]\n",
        "vit_f1 = [vit_report[str(i)]['f1-score'] * 100 for i in range(len(SOIL_CLASSES))]\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(14, 7))\n",
        "\n",
        "x = np.arange(len(classes_short))\n",
        "width = 0.25\n",
        "\n",
        "bars1 = ax.bar(x - width, resnet_f1, width, label='ResNet50', color='#D0D0D0', edgecolor='black', hatch='///')\n",
        "bars2 = ax.bar(x, efficientnet_f1, width, label='EfficientNet-B3', color='#909090', edgecolor='black', hatch='...')\n",
        "bars3 = ax.bar(x + width, vit_f1, width, label='ViT', color='#505050', edgecolor='black', hatch='\\\\\\\\\\\\')\n",
        "\n",
        "ax.set_xlabel('Soil Type', fontsize=12)\n",
        "ax.set_ylabel('F1-Score (%)', fontsize=12)\n",
        "ax.set_title('Per-Class F1-Score Comparison Across CNN Architectures', fontsize=14, fontweight='bold')\n",
        "ax.set_xticks(x)\n",
        "ax.set_xticklabels(classes_short, rotation=45, ha='right')\n",
        "ax.legend(loc='upper right')\n",
        "ax.set_ylim(0, 110)\n",
        "ax.grid(axis='y', alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(f'{CONFIG[\"figures_dir\"]}/RQ1/RQ1_Fig5.pdf', dpi=300, bbox_inches='tight')\n",
        "plt.close()\n",
        "print(\"  Saved: RQ1_Fig5.pdf (per-class F1 comparison)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IabH0JNLpTDL",
        "outputId": "6e77e04d-72b1-4778-eaba-014e479286fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating per-class F1 comparison...\n",
            "  Saved: RQ1_Fig5.pdf (per-class F1 comparison)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# C24c: ROC Curves for Multi-class Classification\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "from sklearn.preprocessing import label_binarize\n",
        "\n",
        "print(\"Creating ROC curves...\")\n",
        "\n",
        "y_test_bin = label_binarize(test_labels, classes=range(len(SOIL_CLASSES)))\n",
        "\n",
        "efficientnet_model.eval()\n",
        "all_probs = []\n",
        "with torch.no_grad():\n",
        "    for images, _ in test_loader:\n",
        "        outputs = efficientnet_model(images.to(device))\n",
        "        probs = torch.softmax(outputs, dim=1)\n",
        "        all_probs.extend(probs.cpu().numpy())\n",
        "y_score = np.array(all_probs)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10, 8))\n",
        "\n",
        "line_styles = ['-', '--', '-.', ':', '-', '--', '-.']\n",
        "colors = ['#000000', '#333333', '#555555', '#777777', '#999999', '#BBBBBB', '#DDDDDD']\n",
        "\n",
        "for i in range(len(SOIL_CLASSES)):\n",
        "    fpr, tpr, _ = roc_curve(y_test_bin[:, i], y_score[:, i])\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "    ax.plot(fpr, tpr, linestyle=line_styles[i], color=colors[i], linewidth=2,\n",
        "            label=f'{SOIL_CLASSES[i].replace(\"_Soil\", \"\")} (AUC = {roc_auc:.2f})')\n",
        "\n",
        "ax.plot([0, 1], [0, 1], 'k--', linewidth=1, label='Random Classifier')\n",
        "ax.set_xlabel('False Positive Rate', fontsize=12)\n",
        "ax.set_ylabel('True Positive Rate', fontsize=12)\n",
        "ax.set_title('ROC Curves for EfficientNet-B3 (Per Class)', fontsize=14, fontweight='bold')\n",
        "ax.legend(loc='lower right', fontsize=9)\n",
        "ax.grid(alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(f'{CONFIG[\"figures_dir\"]}/RQ1/RQ1_Fig6.pdf', dpi=300, bbox_inches='tight')\n",
        "plt.close()\n",
        "print(\"  Saved: RQ1_Fig6.pdf (ROC curves)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tfX35bAYqFad",
        "outputId": "4227eab5-67b1-4bcf-89a2-ebbd814fe758"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating ROC curves...\n",
            "  Saved: RQ1_Fig6.pdf (ROC curves)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "11. Systematic Ablation Analysis"
      ],
      "metadata": {
        "id": "pB2WK4cqcrBf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "11.1 Ablation study summary and final tables"
      ],
      "metadata": {
        "id": "EIO_tX3B3-du"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# C25: Ablation study summary and final tables\n",
        "print(\"Creating ablation study summary\")\n",
        "\n",
        "# Ablation study: 4 configurations\n",
        "ablation_configs = {\n",
        "    'CNN Only': efficientnet_test_acc,\n",
        "    'Metalearner Only': meta_test_acc,\n",
        "    'CNN + Metalearner': meta_test_acc,\n",
        "    'Full Hybrid (CNN + Meta + Rules)': hybrid_test_acc\n",
        "}\n",
        "\n",
        "print(\"\\nAblation Study Results:\")\n",
        "#print(\"=\" * 60)\n",
        "for config, acc in ablation_configs.items():\n",
        "    print(f\"{config:35s}: {acc:.2f}%\")\n",
        "\n",
        "# Create ablation study table\n",
        "ablation_df = pd.DataFrame({\n",
        "    'Configuration': list(ablation_configs.keys()),\n",
        "    'Test Accuracy (%)': [f\"{v:.2f}\" for v in ablation_configs.values()],\n",
        "    'Components': [\n",
        "        'EfficientNet-B3 only',\n",
        "        'RF + XGB + LGB + SVM + Meta-LR',\n",
        "        'CNN features + Ensemble',\n",
        "        'CNN + Ensemble + Rule Engine'\n",
        "    ]\n",
        "})\n",
        "ablation_df.to_excel(f'{CONFIG[\"tables_dir\"]}/RQ3/RQ3_Tab2.xlsx', index=False)\n",
        "print(\"\\nAblation study table saved: RQ3_Tab2.xlsx\")\n",
        "\n",
        "# Create comprehensive results summary\n",
        "summary_data = {\n",
        "    'Research Question': [\n",
        "        'RQ1: CNN Architecture Comparison',\n",
        "        'RQ2: Metalearner Improvement',\n",
        "        'RQ3: Rule-based Integration',\n",
        "        'RQ4: Explainability',\n",
        "        'RQ5: Deployment Feasibility'\n",
        "    ],\n",
        "    'Key Finding': [\n",
        "        f'EfficientNet-B3 best ({efficientnet_test_acc:.2f}%)',\n",
        "        f'Metalearner improved by {meta_test_acc - efficientnet_test_acc:.2f}%',\n",
        "        f'Hybrid system: {hybrid_test_acc:.2f}%',\n",
        "        'GradCAM + SHAP implemented',\n",
        "        f'EfficientNet: {efficientnet_time:.2f}ms, {efficientnet_size:.2f}MB'\n",
        "    ],\n",
        "    'Status': [' Complete'] * 5\n",
        "}\n",
        "\n",
        "summary_df = pd.DataFrame(summary_data)\n",
        "summary_df.to_excel(f'{CONFIG[\"checkpoint_dir\"]}/project_summary.xlsx', index=False)\n",
        "print(\"Project summary saved: project_summary.xlsx\")\n",
        "\n",
        "#print(\"\\n\" + \"=\" * 60)\n",
        "print(\"ALL RESEARCH QUESTIONS COMPLETED!\")\n",
        "#print(\"=\" * 60)\n",
        "print(f\"\\nFinal Results:\")\n",
        "print(f\"  Best CNN: EfficientNet-B3 ({efficientnet_test_acc:.2f}%)\")\n",
        "print(f\"  Best Overall: Hybrid System ({hybrid_test_acc:.2f}%)\")\n",
        "print(f\"  Improvement over baseline: +{hybrid_test_acc - resnet_test_acc:.2f}%\")\n",
        "print(f\"\\nFiles generated:\")\n",
        "print(f\"  Figures: {sum([len(os.listdir(f'{CONFIG['figures_dir']}/RQ{i}')) for i in range(1,6)])} files\")\n",
        "print(f\"  Tables: {sum([len(os.listdir(f'{CONFIG['tables_dir']}/RQ{i}')) for i in range(1,6)])} files\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MVp8H_arcbb0",
        "outputId": "941fa904-21aa-4140-9de9-59e72599e657"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating ablation study summary\n",
            "\n",
            "Ablation Study Results:\n",
            "CNN Only                           : 88.76%\n",
            "Metalearner Only                   : 88.20%\n",
            "CNN + Metalearner                  : 88.20%\n",
            "Full Hybrid (CNN + Meta + Rules)   : 88.20%\n",
            "\n",
            "Ablation study table saved: RQ3_Tab2.xlsx\n",
            "Project summary saved: project_summary.xlsx\n",
            "ALL RESEARCH QUESTIONS COMPLETED!\n",
            "\n",
            "Final Results:\n",
            "  Best CNN: EfficientNet-B3 (88.76%)\n",
            "  Best Overall: Hybrid System (88.20%)\n",
            "  Improvement over baseline: +7.87%\n",
            "\n",
            "Files generated:\n",
            "  Figures: 21 files\n",
            "  Tables: 7 files\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "12 Comprehensive Results & Performance Visualization"
      ],
      "metadata": {
        "id": "W-U2R3Tn41GI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "12.1 Create final submission package (zip figures and tables)"
      ],
      "metadata": {
        "id": "EcCjLQ2O4Twq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# C26: Create final submission package (zip figures and tables)\n",
        "import shutil\n",
        "\n",
        "print(\"Creating final submission package\")\n",
        "\n",
        "# Create zip file with required structure\n",
        "zip_filename = f'{PROJECT_ROOT}/Figures_Tables.zip'\n",
        "\n",
        "# Remove old zip if exists\n",
        "if os.path.exists(zip_filename):\n",
        "    os.remove(zip_filename)\n",
        "\n",
        "# Create zip with proper structure\n",
        "shutil.make_archive(\n",
        "    f'{PROJECT_ROOT}/Figures_Tables',\n",
        "    'zip',\n",
        "    PROJECT_ROOT,\n",
        "    'figures'\n",
        ")\n",
        "\n",
        "# Also zip tables\n",
        "shutil.make_archive(\n",
        "    f'{PROJECT_ROOT}/Tables_temp',\n",
        "    'zip',\n",
        "    PROJECT_ROOT,\n",
        "    'tables'\n",
        ")\n",
        "\n",
        "print(f\"\\nSubmission package created: Figures_Tables.zip\")\n",
        "\n",
        "# Print submission checklist\n",
        "#print(\"\\n\" + \"=\" * 70)\n",
        "print(\"Part 2 Submission Checklist\")\n",
        "#print(\"=\" * 70)\n",
        "print(\"\\n Required deliverables:\")\n",
        "print(\"  1. Jupyter Notebook (.ipynb) - this file\")\n",
        "print(\"  2. GitHub Repository Link - to be provided\")\n",
        "print(\"  3. Figures_Tables.zip with RQ1-RQ5 folders\")\n",
        "print(\"\\n Research Questions:\")\n",
        "print(\"  RQ1: CNN Architecture Comparison (ResNet, EfficientNet, ViT)\")\n",
        "print(\"  RQ2: Metalearner vs CNN Performance\")\n",
        "print(\"  RQ3: Rule-based Integration Impact\")\n",
        "print(\"  RQ4: Explainability (GradCAM + SHAP)\")\n",
        "print(\"  RQ5: Deployment Feasibility (Speed, Size, Calibration)\")\n",
        "print(\"\\n Figures per RQ:\")\n",
        "for i in range(1, 6):\n",
        "    num_figs = len(os.listdir(f'{CONFIG[\"figures_dir\"]}/RQ{i}'))\n",
        "    print(f\"  RQ{i}: {num_figs} figures\")\n",
        "print(\"\\n Tables per RQ:\")\n",
        "for i in range(1, 6):\n",
        "    num_tabs = len(os.listdir(f'{CONFIG[\"tables_dir\"]}/RQ{i}'))\n",
        "    print(f\"  RQ{i}: {num_tabs} tables\")\n",
        "\n",
        "# print(\"\\n\" + \"=\" * 70)\n",
        "# print(\"Next Steps:\")\n",
        "# print(\"=\" * 70)\n",
        "# print(\"1. Upload this notebook to GitHub\")\n",
        "# print(\"2. Create README.md with:\")\n",
        "# print(\"   - Project overview\")\n",
        "# print(\"   - Dataset source (Kaggle)\")\n",
        "# print(\"   - Model architecture description\")\n",
        "# print(\"   - Instructions to reproduce results\")\n",
        "# print(\"3. Submit to Teams:\")\n",
        "# print(\"   - This .ipynb file\")\n",
        "# print(\"   - GitHub repository link\")\n",
        "# print(\"   - Figures_Tables.zip\")\n",
        "# print(\"\\n\" + \"=\" * 70)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HAHfFovUcrnf",
        "outputId": "534eaf68-c950-4250-ff44-09f2786437f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating final submission package\n",
            "\n",
            "Submission package created: Figures_Tables.zip\n",
            "Part 2 Submission Checklist\n",
            "\n",
            " Required deliverables:\n",
            "  1. Jupyter Notebook (.ipynb) - this file\n",
            "  2. GitHub Repository Link - to be provided\n",
            "  3. Figures_Tables.zip with RQ1-RQ5 folders\n",
            "\n",
            " Research Questions:\n",
            "  RQ1: CNN Architecture Comparison (ResNet, EfficientNet, ViT)\n",
            "  RQ2: Metalearner vs CNN Performance\n",
            "  RQ3: Rule-based Integration Impact\n",
            "  RQ4: Explainability (GradCAM + SHAP)\n",
            "  RQ5: Deployment Feasibility (Speed, Size, Calibration)\n",
            "\n",
            " Figures per RQ:\n",
            "  RQ1: 6 figures\n",
            "  RQ2: 3 figures\n",
            "  RQ3: 3 figures\n",
            "  RQ4: 5 figures\n",
            "  RQ5: 4 figures\n",
            "\n",
            " Tables per RQ:\n",
            "  RQ1: 1 tables\n",
            "  RQ2: 1 tables\n",
            "  RQ3: 2 tables\n",
            "  RQ4: 1 tables\n",
            "  RQ5: 2 tables\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "12.2 Save final checkpoint and project metadata"
      ],
      "metadata": {
        "id": "2qpfd6zG5uoI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# C27: Save final checkpoint and project metadata\n",
        "print(\"Saving final checkpoint and metadata\")\n",
        "\n",
        "# Save all model checkpoints in one dictionary (JSON serializable)\n",
        "final_checkpoint = {\n",
        "    'project_info': {\n",
        "        'title': 'Precision Agriculture Soil Health Classification System',\n",
        "        'group_members': ['Saba Sabzevari', 'Isha', '[Student 3]'],\n",
        "        'roles': {\n",
        "            'Student 1 (Technical Lead)': 'Saba Sabzevari',\n",
        "            'Student 2 (Figures & Presentation)': 'Isha',\n",
        "            'Student 3 (Report & Storytelling)': '[Student 3]'\n",
        "        },\n",
        "        'dataset': 'Kaggle Comprehensive Soil Classification Dataset',\n",
        "        'num_classes': 7,\n",
        "        'soil_classes': SOIL_CLASSES\n",
        "    },\n",
        "    'final_results': {\n",
        "        'resnet50': {'val_acc': float(resnet_val_acc), 'test_acc': float(resnet_test_acc), 'test_f1': float(resnet_test_f1)},\n",
        "        'efficientnet_b3': {'val_acc': float(efficientnet_val_acc), 'test_acc': float(efficientnet_test_acc), 'test_f1': float(efficientnet_test_f1)},\n",
        "        'vit': {'val_acc': float(vit_val_acc), 'test_acc': float(vit_test_acc), 'test_f1': float(vit_test_f1)},\n",
        "        'metalearner': {'val_acc': float(meta_val_acc*100), 'test_acc': float(meta_test_acc), 'test_f1': float(meta_test_f1)},\n",
        "        'hybrid': {'val_acc': float(meta_val_acc*100), 'test_acc': float(hybrid_test_acc), 'test_f1': float(hybrid_test_f1)}\n",
        "    },\n",
        "    'research_questions': {\n",
        "        'RQ1': f'EfficientNet-B3 outperformed (Test: {efficientnet_test_acc:.2f}%)',\n",
        "        'RQ2': f'Metalearner improved by {meta_test_acc - efficientnet_test_acc:.2f}%',\n",
        "        'RQ3': f'Hybrid system achieved {hybrid_test_acc:.2f}%',\n",
        "        'RQ4': 'Explainability: GradCAM (3 samples) + SHAP (50 samples)',\n",
        "        'RQ5': f'Deployment: {efficientnet_time:.2f}ms inference, {efficientnet_size:.2f}MB, ECE={ece:.4f}'\n",
        "    },\n",
        "    'config': {\n",
        "        'device': str(device),\n",
        "        'batch_size': CONFIG['batch_size'],\n",
        "        'img_size': CONFIG['img_size'],\n",
        "        'epochs': CONFIG['epochs'],\n",
        "        'learning_rate': CONFIG['learning_rate'],\n",
        "        'num_classes': CONFIG['num_classes']\n",
        "    },\n",
        "    'data_splits': {\n",
        "        'train_size': len(X_train),\n",
        "        'val_size': len(X_val),\n",
        "        'test_size': len(X_test)\n",
        "    }\n",
        "}\n",
        "\n",
        "# Save as JSON for easy reading\n",
        "with open(f'{PROJECT_ROOT}/final_results.json', 'w') as f:\n",
        "    json.dump(final_checkpoint, f, indent=2)\n",
        "\n",
        "print(\" Final checkpoint saved: final_results.json\")\n",
        "\n",
        "# Create README content for GitHub\n",
        "readme_content = f\"\"\"# Precision Agriculture Soil Health Classification System\n",
        "\n",
        "## Project Overview\n",
        "Hybrid expert system combining CNNs, metalearning, and rule-based reasoning for soil type classification from images.\n",
        "\n",
        "**Course:** Pattern Recognition (M.Sc.) - Winter Semester 2025\n",
        "**Instructor:** Prof. Raja Hashim Ali\n",
        "**Group Members:**\n",
        "- **Student 1 (Technical Lead):** Saba Sabzevari\n",
        "- **Student 1 (Figures & Presentation):** Saba Sabzevari\n",
        "- **Student 3 (Report & Storytelling):** Isha\n",
        "\n",
        "## Dataset\n",
        "- **Source:** [Kaggle Comprehensive Soil Classification Dataset](https://www.kaggle.com/datasets/ai4a-lab/comprehensive-soil-classification-datasets)\n",
        "- **Classes:** 7 soil types (Alluvial, Arid, Black, Laterite, Mountain, Red, Yellow)\n",
        "- **Total Images:** 1,186\n",
        "- **Split:** 70% train, 15% val, 15% test (stratified)\n",
        "\n",
        "## Model Architecture\n",
        "\n",
        "### 1. CNN Feature Extraction (RQ1)\n",
        "Three architectures compared:\n",
        "- **ResNet50** (2048-dim features)\n",
        "- **EfficientNet-B3** (1536-dim features) ✓ Best\n",
        "- **Vision Transformer** (768-dim features)\n",
        "\n",
        "### 2. Metalearner Ensemble (RQ2)\n",
        "Stacking ensemble:\n",
        "- Base learners: Random Forest, XGBoost, LightGBM, SVM\n",
        "- Meta-learner: Logistic Regression\n",
        "\n",
        "### 3. Rule-based Engine (RQ3)\n",
        "Agricultural domain knowledge:\n",
        "- Confidence thresholding\n",
        "- Agreement validation\n",
        "- Crop recommendations\n",
        "\n",
        "## Results\n",
        "\n",
        "| Configuration | Test Accuracy | Test F1-Score |\n",
        "|--------------|---------------|---------------|\n",
        "| ResNet50 | {resnet_test_acc:.2f}% | {resnet_test_f1:.2f}% |\n",
        "| EfficientNet-B3 | {efficientnet_test_acc:.2f}% | {efficientnet_test_f1:.2f}% |\n",
        "| Vision Transformer | {vit_test_acc:.2f}% | {vit_test_f1:.2f}% |\n",
        "| Metalearner | {meta_test_acc:.2f}% | {meta_test_f1:.2f}% |\n",
        "| **Hybrid System** | **{hybrid_test_acc:.2f}%** | **{hybrid_test_f1:.2f}%** |\n",
        "\n",
        "## Explainability (RQ4)\n",
        "- **GradCAM:** Visual attention maps for CNN predictions\n",
        "- **SHAP:** Feature importance for metalearner\n",
        "\n",
        "## Deployment (RQ5)\n",
        "- **Inference Time:** {efficientnet_time:.2f} ms/batch (CPU)\n",
        "- **Model Size:** {efficientnet_size:.2f} MB\n",
        "- **Calibration:** ECE = {ece:.4f}\n",
        "\n",
        "## Reproduce Results\n",
        "\n",
        "### Requirements\n",
        "```bash\n",
        "pip install torch torchvision scikit-learn xgboost lightgbm opencv-python shap\n",
        "```\n",
        "\n",
        "### Run\n",
        "1. Mount Google Drive with dataset\n",
        "2. Run all cells in `Soil_Classification.ipynb`\n",
        "3. Checkpoints saved automatically - rerun uses cached models\n",
        "\n",
        "### File Structure\n",
        "```\n",
        "Soil_Health_Classification_Phase2/\n",
        "├── checkpoints/          # Trained models\n",
        "├── figures/             # RQ1-RQ5 figures (PDF)\n",
        "├── tables/              # RQ1-RQ5 tables (Excel)\n",
        "└── dataset/             # Soil images\n",
        "```\n",
        "\n",
        "## References\n",
        "- Dataset: [Kaggle Comprehensive Soil Classification](https://www.kaggle.com/datasets/ai4a-lab/comprehensive-soil-classification-datasets)\n",
        "- EfficientNet: Tan & Le (2019)\n",
        "- SHAP: Lundberg & Lee (2017)\n",
        "- GradCAM: Selvaraju et al. (2017)\n",
        "\"\"\"\n",
        "\n",
        "with open(f'{PROJECT_ROOT}/README.md', 'w') as f:\n",
        "    f.write(readme_content)\n",
        "\n",
        "print(\" README.md created for GitHub\")\n",
        "\n",
        "#print(\"\\n\" + \"=\" * 70)\n",
        "print(\"Priject Completed\")\n",
        "#print(\"=\" * 70)\n",
        "print(f\"\\nAll files saved in: {PROJECT_ROOT}\")\n",
        "#print(\"\\nReady for submission\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xRXtFHWFc4ZR",
        "outputId": "738ef927-e3de-4ade-f57a-6f343c1a8ffb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving final checkpoint and metadata\n",
            " Final checkpoint saved: final_results.json\n",
            " README.md created for GitHub\n",
            "Priject Completed\n",
            "\n",
            "All files saved in: /content/drive/MyDrive/Soil_Health_Classification_Phase2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "12.3 Final summary and submission instructions"
      ],
      "metadata": {
        "id": "OhjoEeRd6X6A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# C28: Final summary and submission instructions\n",
        "#print(\"=\" * 70)\n",
        "print(\"Precision Agriculture Soil Health Classification - Phase 2\")\n",
        "#print(\"=\" * 70)\n",
        "\n",
        "print(\"\\nFinal Results Summary:\")\n",
        "#print(\"-\" * 70)\n",
        "print(f\"Research Question 1 (CNN Comparison):\")\n",
        "print(f\"  - ResNet50: {resnet_test_acc:.2f}%\")\n",
        "print(f\"  - EfficientNet-B3: {efficientnet_test_acc:.2f}%  Best CNN\")\n",
        "print(f\"  - Vision Transformer: {vit_test_acc:.2f}%\")\n",
        "print(f\"\\nResearch Question 2 (Metalearner):\")\n",
        "print(f\"  - Improvement: +{meta_test_acc - efficientnet_test_acc:.2f}%\")\n",
        "print(f\"  - Test Accuracy: {meta_test_acc:.2f}%\")\n",
        "print(f\"\\nResearch Question 3 (Hybrid System):\")\n",
        "print(f\"  - Final Accuracy: {hybrid_test_acc:.2f}%\")\n",
        "print(f\"  - Total Improvement: +{hybrid_test_acc - resnet_test_acc:.2f}% over baseline\")\n",
        "print(f\"\\nResearch Question 4 (Explainability):\")\n",
        "print(f\"  - GradCAM visualizations: 3 samples\")\n",
        "print(f\"  - SHAP analysis: 50 samples\")\n",
        "print(f\"\\nResearch Question 5 (Deployment):\")\n",
        "print(f\"  - Inference speed: {efficientnet_time:.2f} ms/batch\")\n",
        "print(f\"  - Model size: {efficientnet_size:.2f} MB\")\n",
        "print(f\"  - Calibration ECE: {ece:.4f}\")\n",
        "\n",
        "print(\"\\nFiles Generated\")\n",
        "#print(\"-\" * 70)\n",
        "print(f\"Checkpoint directory: {CONFIG['checkpoint_dir']}\")\n",
        "print(f\"  - Model checkpoints: 3 CNNs + metalearner\")\n",
        "print(f\"  - Data splits saved for reproducibility\")\n",
        "print(f\"\\nFigures directory: {CONFIG['figures_dir']}\")\n",
        "\n",
        "# Count methodology figures (in main figures folder)\n",
        "methodology_figs = len([f for f in os.listdir(CONFIG['figures_dir'])\n",
        "                        if f.endswith('.pdf') and not os.path.isdir(os.path.join(CONFIG['figures_dir'], f))])\n",
        "print(f\"  - Methodology: {methodology_figs} PDF figures\")\n",
        "\n",
        "for i in range(1, 6):\n",
        "    num_figs = len(os.listdir(f'{CONFIG[\"figures_dir\"]}/RQ{i}'))\n",
        "    print(f\"  - RQ{i}: {num_figs} PDF figures\")\n",
        "print(f\"\\nTables directory: {CONFIG['tables_dir']}\")\n",
        "for i in range(1, 6):\n",
        "    num_tabs = len(os.listdir(f'{CONFIG[\"tables_dir\"]}/RQ{i}'))\n",
        "    print(f\"  - RQ{i}: {num_tabs} Excel tables\")\n",
        "\n",
        "# print(\"\\n SUBMISSION CHECKLIST:\")\n",
        "# print(\"-\" * 70)\n",
        "# print(\" Jupyter Notebook (.ipynb) - THIS FILE\")\n",
        "# print(\" Figures_Tables.zip - Created\")\n",
        "# print(\" GitHub Repository Link - TODO: Upload and share link\")\n",
        "# print(\" All 5 Research Questions answered\")\n",
        "# print(\" Checkpointing implemented (can rerun without retraining)\")\n",
        "\n",
        "# print(\"\\n Next Actions:\")\n",
        "# print(\"-\" * 70)\n",
        "# print(\"1. Download this notebook (.ipynb file)\")\n",
        "# print(\"2. Upload notebook + all checkpoints to GitHub\")\n",
        "# print(\"3. Add README.md (already created in project folder)\")\n",
        "# print(\"4. Submit to Microsoft Teams (ONLY Student 1):\")\n",
        "# print(\"   - This .ipynb file\")\n",
        "# print(\"   - GitHub repository link\")\n",
        "# print(\"   - Figures_Tables.zip\")\n",
        "# print(\"\\nDeadline: January 04, 11:59 PM\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BSp4QHucdSDn",
        "outputId": "8efd573d-d2c7-4857-e9b7-e41eed2b31a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision Agriculture Soil Health Classification - Phase 2\n",
            "\n",
            "Final Results Summary:\n",
            "Research Question 1 (CNN Comparison):\n",
            "  - ResNet50: 80.34%\n",
            "  - EfficientNet-B3: 88.76%  Best CNN\n",
            "  - Vision Transformer: 74.16%\n",
            "\n",
            "Research Question 2 (Metalearner):\n",
            "  - Improvement: +-0.56%\n",
            "  - Test Accuracy: 88.20%\n",
            "\n",
            "Research Question 3 (Hybrid System):\n",
            "  - Final Accuracy: 88.20%\n",
            "  - Total Improvement: +7.87% over baseline\n",
            "\n",
            "Research Question 4 (Explainability):\n",
            "  - GradCAM visualizations: 3 samples\n",
            "  - SHAP analysis: 50 samples\n",
            "\n",
            "Research Question 5 (Deployment):\n",
            "  - Inference speed: 42.65 ms/batch\n",
            "  - Model size: 41.18 MB\n",
            "  - Calibration ECE: 0.0649\n",
            "\n",
            "Files Generated\n",
            "Checkpoint directory: /content/drive/MyDrive/Soil_Health_Classification_Phase2/checkpoints\n",
            "  - Model checkpoints: 3 CNNs + metalearner\n",
            "  - Data splits saved for reproducibility\n",
            "\n",
            "Figures directory: /content/drive/MyDrive/Soil_Health_Classification_Phase2/figures\n",
            "  - Methodology: 2 PDF figures\n",
            "  - RQ1: 6 PDF figures\n",
            "  - RQ2: 3 PDF figures\n",
            "  - RQ3: 3 PDF figures\n",
            "  - RQ4: 5 PDF figures\n",
            "  - RQ5: 4 PDF figures\n",
            "\n",
            "Tables directory: /content/drive/MyDrive/Soil_Health_Classification_Phase2/tables\n",
            "  - RQ1: 1 Excel tables\n",
            "  - RQ2: 1 Excel tables\n",
            "  - RQ3: 2 Excel tables\n",
            "  - RQ4: 1 Excel tables\n",
            "  - RQ5: 2 Excel tables\n"
          ]
        }
      ]
    }
  ]
}